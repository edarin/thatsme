[
  {
    "path": "posts/2025-08-15-bayesian-modeling-in-the-drc/",
    "title": "Modelling population in Kasaï-Oriental, DRC",
    "description": "We published a new study uses satellite data and Bayesian modeling and a novel building model component to estimate population in Kasaï‑Oriental, DRC, supporting better local health planning.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2025-08-15",
    "categories": [
      "academia",
      "paper"
    ],
    "contents": "\r\nReliable population data is essential for public health planning, yet in many regions, especially in low- and middle-income countries, census data is outdated or missing. This creates challenges for everything from vaccine delivery to health resource allocation.\r\nWe recently published in PLOS Global Public Health a method to estimate populations in these data-scarce settings using a Bayesian statistical model. We applied the approach in Kasaï‑Oriental province in the Democratic Republic of the Congo, combining household survey data with satellite-derived settlement data to produce fine-scale population estimates.\r\n\r\nHighlights\r\nThe model estimates 4.1 million people living in the province in 2024, with an uncertainty range of 3.4 to 4.8 million.\r\nEstimates are produced at a 1-hectare resolution, then aggregated to health zones and areas.\r\nThe method accounts for data uncertainty and provides credible intervals, offering more transparency than traditional mapping methods.\r\nAccuracy metrics indicate the model performs well even in areas with limited ground data.\r\nThis approach is particularly useful in contexts where ground data collection is difficult or inconsistent. The results can support more targeted public health strategies and better planning at subnational levels.\r\nNovelty\r\nWe implemented a model component for debiasing building footprint data derived from satellite imagery by integrating true building count observed on the ground.\r\n\r\nRead the full paper\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-08-15-bayesian-modeling-in-the-drc/images/clipboard-182869930.png",
    "last_modified": "2025-09-10T18:15:52+01:00",
    "input_file": {},
    "preview_width": 1712,
    "preview_height": 1500
  },
  {
    "path": "posts/2025-08-01-can-satellites-replace-censuses-what-we-learned-from-colombia/",
    "title": "Can Satellites Replace Censuses? What We Learned from Colombia",
    "description": "How accurate is satellite data for estimating population where censuses can't reach? We benchmark satellite-derived models in Colombia—and reveals when machine learning falls short and why Bayesian models matter.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2025-08-01",
    "categories": [
      "academia",
      "paper"
    ],
    "contents": "\r\nIn a world where more than a quarter of countries haven’t conducted a census in over 10 years, the need for alternative ways to estimate population is urgent. From conflict zones to remote rainforests, reliable data is often out of reach—but decisions on healthcare, education, disaster relief, and infrastructure still need to be made.\r\nOur new study, published in Population, Space and Place, explores one of the most promising alternatives: using satellite imagery to estimate population counts in data-scarce regions.\r\n\r\nWhy This Matters\r\nTraditional censuses are expensive, logistically complex, and sometimes simply not possible—especially in areas affected by violence, displacement, or lack of infrastructure. In such contexts, satellite imagery has become an increasingly attractive solution. But how accurate is it?\r\nTo find out, we used the 2018 Colombian census—one of the most complete and detailed in the world—as a testing ground. We compared different satellite-derived settlement maps and modeling approaches to see how well they predicted actual population counts.\r\nWhat We Tested\r\nWe compared:\r\nSix settlement maps, including building footprints from Google and Microsoft, and pixel-based “built-up area” maps.\r\nTwo modeling approaches:\r\nA Bayesian probabilistic model, which can incorporate uncertainty and adjust for bias.\r\nA random forest machine learning model, commonly used for pattern recognition in data-rich settings.\r\n\r\nKey Findings\r\nBuilding footprints are best: Maps that show individual buildings (like those from Google and Microsoft) were the most accurate for estimating population, especially in urban areas.\r\nBayesian models win in tough settings: When data was sparse, biased, or incomplete—as it often is in remote or forested regions—Bayesian models outperformed machine learning.\r\nAggregated results are more reliable: Predictions were more accurate at larger spatial scales (e.g., municipalities) than at fine-grained levels like individual neighborhoods.\r\nRemote regions are still hard: Accuracy dropped significantly in regions like the Amazon and Pacific coast, where buildings are harder to detect from above.\r\nTake home message: In data-scarce settings, we can’t rely on standard algorithms alone—probabilistic models are essential to correct for bias and uncertainty.\r\nWhat This Means Going Forward\r\nAs more organizations rely on satellite data for planning and policy, our research highlights some critical lessons:\r\nOpen, high-resolution building data is essential.\r\nStatistical models need to be adapted for local realities, especially in contexts where conventional data is lacking or biased.\r\nGround-truthing with even small samples can dramatically improve model performance.\r\nWant to dive deeper? Read the full paper\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-08-01-can-satellites-replace-censuses-what-we-learned-from-colombia/images/clipboard-303943579.png",
    "last_modified": "2025-09-10T17:58:43+01:00",
    "input_file": {},
    "preview_width": 693,
    "preview_height": 269
  },
  {
    "path": "posts/2025-08-12-nowpop-network/",
    "title": "Population nowcasting network",
    "description": "We are building a collaboration between universities for advancing knowledge of nowcasting populations, mobility, and migration by combining Bayesian and other methodologies with digital trace and traditional data sources",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2025-02-01",
    "categories": [
      "academia",
      "engagement"
    ],
    "contents": "\r\nOriginal source: https://www.demography.ox.ac.uk/collaborative-group-nowcasting-populations-cgnp\r\nThanks to funding from Oxford University’s Van Houten Fund, Dr Andrea Aparicio Castro and the Nowcasting Populations (NowPop) team at the Leverhulme Centre for Demographic Science (LCDS) have established the Collaborative Group on Nowcasting Populations (CGNP) to foster collaboration among leading academic institutions and research centres in demographic science.\r\nThe CGNP initiative aims to advance knowledge of nowcasting populations, mobility, and migration by combining Bayesian and other methodologies with digital trace and traditional data sources. The groups work addresses critical challenges in contexts where conventional data collection is limited, such as conflict zones, and provides timely insights to support humanitarian responses.\r\nA key project goal of the initiative is to build and strengthen a community focused on real-time estimation or nowcasting population and mobility, using traditional and digital trace data. Another goal is to establish a foundation for collaborative work and potential application for large grants by exploring and defining common themes.\r\nThe CGNP initiative is led by LCDS’s NowPop project team that drives innovation for near real-time population estimation and involves researchers from Oxford University’s Department of Sociology. This initiative is also a collaboration with teams led by Professor Arkadiusz Wiśniowski at the University of Manchester’s Department of Social Statistics, Professor Jakub Bijak and Dr Jason Hilton at the University of Southampton, and Professor Francisco Rowe, Dr Elisabetta Pietrostefani and Dr Carmen Cabrera-Arnau at the University of Liverpool’s Geographic Data Science Lab.\r\nKey activities of the group include collaborative development sessions conducted at the start and conclusion of the project to align on themes and refine research proposals, expert workshops to identify shared research interests and explore opportunities for sustainable grant applications, and training sessions to enhance skills in Bayesian statistical demography and digital data analysis across partner institutions.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-08-12-nowpop-network/images/clipboard-478357526.png",
    "last_modified": "2025-09-10T17:15:28+01:00",
    "input_file": {},
    "preview_width": 1837,
    "preview_height": 679
  },
  {
    "path": "posts/2025-08-12-ukraine-nowpop-project/",
    "title": "Ukraine NowPop: internal displacement in Ukraine",
    "description": "I am currently working as a population data scientist on the NowPop project, a large-scale population model for Ukraine, using social media and mobile phone data to nowcast population displacement since the full-scale invasion in February 2022.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2024-05-02",
    "categories": [
      "academia",
      "overview"
    ],
    "contents": "\r\nBackground\r\nThe Russian invasion of Ukraine resulted in the forced displacement of millions of Ukrainian residents, including nearly 8 million refugees and 6.5 million internally displaced persons. Plus, there are millions more who remained in place and are now vulnerable due to insecurity and losses of critical infrastructure and services. Up-to-date population figures are essential for assessing needs of populations inside Ukraine, however, pre-conflict official statistics are out-of-date due to large scale migrations, and survey-based primary data collection has not been able to fill this data gap. The Leverhulme Centre for Demographic Science has been supporting the international humanitarian response by providing daily sub-national population estimates for Ukraine disaggregated by 5-year age-sex groups that are produced using real-time counts of active Facebook users combined with baseline population estimates and daily counts of international border crossings (see Leasure et al. 2022; https://doi.org/10.31235/osf.io/6j9wq).\r\nProblem statement\r\nWhile previous approach continues to provide a critical source of up-to-date demographic data, there are a number of shortcomings. We cannot currently incorporate additional sources of population information that would likely improve the precision of demographic estimates, particularly in Donetsk and Luhansk Oblasts where Facebook is used a lower rate and where power outages interrupted the data stream of social media user counts. Other data sources would include social media platforms Instagram and Vkontakte as well as more traditional data like Ukrainian IDP registries and IOM’s General Population Survey. In addition, the current method cannot quantify uncertainty associated with demographic estimates which would be provided by a more rigorous statistical approach. Lastly, we lacked an effective data dissemination platform to provide easy access to our daily population estimates for immediate operational uses.\r\nResearch streams\r\nBayesian Statistical Modelling\r\nWe are transitioning our current rapid-response method into a more rigorous statistical framework that will allow us to combine multiple imperfect observations of population (e.g. various social media platforms, household surveys). This new statistical approach aims to better account for biases and data gaps to increase prediction accuracy and provide robust estimates of uncertainty. This is a critical next step to move from acute crisis response to sustained support for official statistics in Ukraine. The proposed methodology follows a Bayesian timeseries modelling framework to estimate daily estimates of population and internal migration rates at the Oblast level based on social media and other geospatial data. The time series approach provides increased precision by constraining likely population sizes today based on what the population sizes were yesterday, as well as by incorporating additional geospatial data (e.g. travel distances, conflict locations). This also quantifies weekly internal migration probabilities based on events on-the-ground that could inform forecasts of potential future displacement trends, although the proposed work will focus on nowcasting rather than forecasting. W\r\nDigital Demography and Data Science.\r\nThe social media data that underly our approach require large-scale automated data collections running on a daily basis. Previous work was restricted to the Facebook platform, but we have extended this to include Instagram which is more popular among younger age groups. We have also gained access to Vodafone user database which contains monthly counts by age and sex of flows and stocks of mobile phone users at hromada level (administative level 3).\r\nThis will reduce dependency on any single data source and help to fill critical data gaps, particularly at lower geographic scale. We will also incorporate various geospatial datasets that will include remotely-sensed buildings, road networks, and daily geolocated conflict events.\r\nData Dissemination\r\nWe are developing an interactive web mapping application to provide instant access to our real-time demographic estimates to better support a broad range of stakeholders. This allows users to click sub-national areas on the map to retrieve current and historical population estimates for specific age-sex demographic groups along with robust measures of statistical estimation uncertainty.\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-08-12-ukraine-nowpop-project/dashboard.png",
    "last_modified": "2025-09-10T17:16:19+01:00",
    "input_file": {},
    "preview_width": 1153,
    "preview_height": 869
  },
  {
    "path": "posts/2023-07-01-collaboration-with-the-colombia-statistics-office/",
    "title": "Collaboration with the Colombia Statistics Office",
    "description": "The Leverhulme Centre for Demographic Science is partnering with Colombia’s Stats Office to rethink how population counts are produced between censuses. By combining Bayesian modelling, administrative records, and satellite data, the project is pioneering new methods to generate accurate annual population estimates.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2023-07-01",
    "categories": [
      "academia",
      "engagement"
    ],
    "contents": "\r\nThe Leverhulme Centre for Demographic Science is building a partnership\r\nwith the National Administrative Department of Statistics of Colombia\r\n(DANE) to work together to address the challenge of outdated population\r\ncount in Colombia, paving the way for other national statistics offices\r\nto follow.\r\nThrough our partnership with DANE, we are addressing the following\r\nquestion: Can we predict the demographics that a national census would\r\nobserve if it were conducted today?\r\nThis project is designed to build the foundation to answer this question\r\nby achieving four objectives:\r\nBuild knowledge at DANE to implement Bayesian statistical modelling,\r\nInstall necessary computing infrastructure at DANE,\r\nImplement a pilot project as a proof-of-concept to “predict the\r\ncensus”, and\r\nFoster new relationships for broader collaborations between Oxford\r\nand DANE.\r\nOur ultimate aim is to produce annual population estimates for official\r\nuse by the Colombian government and to share these approaches with a\r\nnetwork of national statistics offices globally through our partnership\r\nwith the United Nations Population Fund (UNFPA).\r\nWe conducted conduct a one week capacity-building workshop at DANE in\r\nBogotá, Colombia in partnership with UNFPA. We taught fundamentals of\r\nBayesian statistics to representatives from 5 national statistics\r\noffices in Latin America, targeting methods to produce annual small area\r\npopulation estimates using the 2018 Colombian census, annual\r\nadministrative registers, geospatial data, and social media data. We\r\nthen invited DANE researchers to visit Oxford for a one week knowledge\r\nexchange workshop intended to spark additional collaborations and expand\r\nthe partnership between our institutions to include a broader range of\r\ntopics and diversity of researchers.\r\n\r\nBuilding on this visit, we worked on two different research projects.\r\nThe first one consist on using confidential Colombian census data as a\r\nbenchmark for testing satellite-imagery derived population. The results\r\nhave been\r\npublished and\r\nhave shown that detailed building footprint data significantly improves\r\naccuracy of population model, and that Bayesian models are more\r\neffective than machine learning when the input data is biased.\r\nThe second research strand consists in modelling intercensal population\r\nby leveraging the signals from administrative records to update the 2018\r\ncensus. I have accessed to anonymized administrative data for 50 million\r\nColombians in 52 administrative databases. I am currently working on a\r\nstate-space model, where the process model is based on the cohort\r\ncomponent framework for population projecton that are nowcasted through\r\nobservation models based on the administrative records.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-01-collaboration-with-the-colombia-statistics-office/images/clipboard-2983698905.png",
    "last_modified": "2025-09-10T17:15:04+01:00",
    "input_file": {},
    "preview_width": 1380,
    "preview_height": 510
  },
  {
    "path": "posts/2023-02-01-mapping-refugees-in-cameroon-with-high-resolution/",
    "title": "Mapping refugees in Cameroon with high-resolution",
    "description": "In collaboration with UN High Commissioner for refugees, we innovated the mapping of displaced populaions by unlocking the power of registration data using satellite imagery and advanced spatial modelling",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2023-02-01",
    "categories": [
      "academia",
      "engagement"
    ],
    "contents": "\r\nThe text comes from a Medium article published by the UNHCR Innovation service.\r\nAs global displacement continues to rise, accurate data collection on people forced to flee has become a critical aspect of humanitarian aid. Since 2003, UNHCR, the UN Refugee Agency, has maintained proGres, its official population registration tool, to manage the personal data of forcibly displaced and stateless people. While this tool is essential for day-to-day humanitarian work, it is also becoming a key source of demographic information, which can act as a master sampling frame for conducting surveys that help assess the living conditions of these vulnerable populations. However, despite its importance, data from proGres has its limitations, particularly when it comes to extracting geographic information to map the locations of refugees, especially those living outside of formal settlements.\r\nTo tackle this challenge, UNHCR partnered with WorldPop, a research group at the University of Southampton, to explore the potential of combining registration data from proGres with satellite imagery to build more accurate sampling frames — lists of all individuals within specific areas, used to identify potential survey respondents. This project, supported by UNHCR’s Data Innovation Fund, seeks to address a critical issue: improving the process of mapping and surveying refugees, especially in out-of-camp contexts where populations are highly dispersed.\r\nThe challenge: Sampling in complex humanitarian settings\r\nTraditional survey methods involve a multi-stage sampling process, where enumeration areas (small geographical areas) are randomly selected for surveys, and then individuals are drawn from these areas. However, in the context of displaced populations, particularly those living outside formal settlements, this approach becomes impractical due to the rapid obsolescence of sampling frames that were designed before the displacement crisis and do not account for new movements. Even when sampling frames designed for standard household surveys are available, displaced populations often represent a small share of the overall population, which means that most sampled areas would contain zero displaced persons, making the process costly and inefficient. What is needed, then, are sampling frames designed specifically for studying displaced populations.\r\n\r\nRefugee population by province in Cameroon as of April 2023. Source: proGres Registration Database, 2023.\r\nThis project originated from the challenges faced by UNHCR regional statisticians in West and Central Africa, Southern Africa, and the Americas when conducting surveys of refugees using traditional sampling methods, since gathering reliable information about the whereabouts of out-of-camp refugees requires substantial resources. In 2021, these statisticians (who are some of the authors of this piece) spotted a potential opportunity in an important paper authored by Sarchil Qader and other members of the WorldPop research group. This paper outlined a method to semi-automate the delineation of enumeration areas, which could then be used to conduct more accurate surveys. The success of these researchers in handling complex population mapping inspired the UNHCR statisticians to reach out to the lead author to explore a possible collaboration around the production of a survey sampling frame for refugees in Cameroon. The potential was clear: they could combine UNHCR refugee registration data with WorldPop mapping expertise to create something new.\r\nThe solution: Leveraging satellite imagery and advanced modelling\r\nIn this project, UNHCR and WorldPop set out to explore whether novel data sources — such as satellite imagery and digital traces — combined with advanced statistical techniques and spatial data modelling could improve the creation of sampling frames.\r\nOne of the primary goals was to develop a gridded map of predicted refugee populations at a fine scale, as demonstrated in the Cameroon-focused pilot project. As of April 2023, UNHCR had registered 471,400 refugees in Cameroon. By combining proGres data with high-resolution satellite imagery, the collaboration aimed to more accurately map refugee populations, to provide better data for survey planning and humanitarian operations.\r\n\r\nMethodological framework for grid-based refugee mapping: A two-stage workflow. Source: proGres Registration Database, 2023.\r\nKey outcomes\r\nGridded maps for refugee population planning: The project’s flagship outcome was a 100m-resolution gridded map of predicted refugee populations in Cameroon, covering both camp and out-of-camp settings. This map can guide survey teams to focus on areas where refugees are likely to be concentrated, thus optimizing survey efforts and helping UNHCR and its partners direct resources more effectively. The map is not intended to provide precise population estimates at the grid-cell level, but rather to indicate the relative spatial distribution of refugees, highlighting areas with higher densities.\r\nUsing proGres data to map refugee locations: This initiative delivered a better understanding of the spatial information contained in the proGres database. For example, 83% of registered refugees could be successfully located by cross-referencing proGres data with OpenStreetMap. This highlights the potential for further integration of digital spatial information like OpenStreetMap into UNHCR’s existing data pipelines, to improve the geographic detail of refugee data.\r\nMapping refugee settlements using satellite imagery: Another deliverable of the project was a survey of the different products that could be used for mapping refugee camp layouts using satellite imagery. The highest accuracy was achieved using a building footprint dataset developed by Maxar Technologies and Ecopia AI. This data provides a clear view of refugee settlement patterns and can be complemented by manually mapping new settlements in dynamic settings, where population are moving fast and therefore camps layouts are rapidly changing.\r\nIn addition, this collaboration has underscored the value of the free-text fields in the proGres database which contain detailed location descriptions provided orally by refugees (given the lack of a unified address system). The project found that this data could be improved by standardising location details and increasing the usage of digital spatial tools such as OpenStreetMap.\r\nLimitations and cautions\r\nWhile the project produced unprecedented insights into the locations of displaced persons across Cameroon, there are some limitations to the applied methodology:\r\nSparse mapping in low refugee areas: In administrative units with few reported refugees, the mapping can produce sparse, low-density estimates, which may dilute the actual presence of refugees in those areas.\r\nDependency on population map accuracy: The gridded refugee map relies heavily on the accuracy of the underlying population map. If inaccuracies exist in the population data, certain areas with refugee populations could be overlooked.\r\nFurthermore, given the sensitivity of this data, the importance of using refugee maps responsibly must be emphasized. While the gridded maps provide valuable insights, there is potential for them to be misused for detrimental policies that target refugees. One solution is to aggregate the data at a broader scale or to restrict access to it, to ensure it is used only for humanitarian purposes.\r\nLooking ahead: Smarter surveys, better aid\r\nThe collaboration between UNHCR and WorldPop represents a significant step forward in the use of data for humanitarian efforts. By combining traditional refugee registration data with cutting-edge technology, including satellite imagery and advanced spatial modelling, this project has laid the groundwork for smarter, more targeted surveys.\r\nWith the estimates of refugees and asylum-seekers per grid cell allowing flexibility and comparability, this method is suitable for a wide range of use cases. The resulting data will provide invaluable insights in contexts where data disaggregation by sex and age is not currently available. This data could, for example, support targeted programme interventions, as well as acting as source data for other projects using statistical and machine learning techniques.\r\nAs a result, humanitarian agencies can better understand the needs of people forced to flee, even in dispersed and hard-to-reach locations. As this innovative approach continues to evolve, it promises to enhance humanitarian response efforts, ensuring that the most vulnerable populations receive the support they need, wherever they may be.\r\nRead the full paper produced through this collaboration here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-01-mapping-refugees-in-cameroon-with-high-resolution/images/clipboard-61555544.webp",
    "last_modified": "2025-09-10T17:59:49+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-01-radical-routes/",
    "title": "Radical Routes",
    "description": "I am supporting the UK network of mutually-owned housing cooperatives called Radical Routes and more speficially the Finance Working Group.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2023-01-01",
    "categories": [
      "cooperative"
    ],
    "contents": "\r\n\r\nFor more info, see here.\r\nRadical Routes is a network of co-ops and individuals seeking to see a world based on equality and co-operation, where people give according to their ability and receive according to their needs, where work is fulfilling and useful and creativity is encouraged, where decision making is open to everyone with no hierarchies, where the environment is valued and respected in its own right rather than exploited.\r\nWe are working towards taking control over our housing, education and work through setting up housing and worker co-ops, and co-operating as a network.\r\nThe specific means it is pursuing are:\r\nThe setting up of housing co-ops to house people and projects with the above aims.\r\nThe setting up of workers co-ops which operate with the above aims.\r\nThe promotion and organisation of participatory education through skills- and knowledge-sharing events, Taking Control events, informative material and workshops.\r\nThe raising of finance to take control over resources (property, technology, land…) through co-operation and economic interlocking of the co-ops.\r\nThe support of like-minded projects.\r\nI specifically participate to the Finance Working Group which reviews loan applications, supports co-ops at every stage of setting up their business, including raising finance and doing financial planning with our cash-flow forecast modeling spreadsheet and runs a participatory budget setting for Radical Routes.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-01-radical-routes/images/clipboard-2824881826.jpeg",
    "last_modified": "2025-09-10T18:15:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-24-sicss-covenantusing-gridded-population-to-gain-spatial-insights-in-r/",
    "title": "SICSS-Covenant:Using gridded population to gain spatial insights in R",
    "description": "I have been invited to present at the Summer Institute for Computational Social Sciences in University of Covenant. My talks starts is about gridded population as a tool to link  demographics and geography. It starts with a global overview of the definition, production and use of this innovative data and pursues with a hands-on in R to gain spatial insights on demography in Nigeria.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2022-06-24",
    "categories": [],
    "contents": "\r\n\r\nExtract from the github repo that contains the session’s material.\r\n\r\nLinking demographics and geography: Using gridded population to gain spatial insights in R\r\nIntroduction\r\nAccess to high-resolution population counts is key for local, national and international decision-making and intervention. It supports data-driven planning of critical infrastructures, such as schools, health facilities and transportation networks.\r\nWorldPop has developed modelling techniques to estimate population in grid cells of 100m by 100m by disaggregating census-based population totals for the entire world, leveraging the growing availability of products derived from satellite imagery. This level of detail offers the advantage of flexible aggregation of the population estimates within different administrative and functional units, for instance, school catchment areas and health zones.\r\nThis session will cover the notion of gridded population, a data format at the crossroad of demography and geography. We will then have a brief overview of openly available satellite-imagery-based products that can be used for modelling gridding population and beyond, such as settlement maps. Finally, we will have some hands-on to extract information from a gridded population covering the following R packages for geospatial analysis: sf (Pebesma, E., 2018), raster (Hijmans, R., 2021), and tmap (Tennekes, M., 2018).\r\nChallenge\r\nWe will study the question: How many women of childbearing age are struggling to access maternal health services?\r\n\r\nConcepts\r\nThis tutorial covers the concepts of:\r\ninteractive mapping\r\nvector file reading and filtering\r\nraster file reading\r\nbuffering\r\nrasterising\r\nzonal statistics\r\nmasking\r\nContents\r\nOn github is stored the raw material for this tutorial. The script for_students.R contains the workflow with the questions. The script teaching.R contains the workflow with the answers. The powerpoint SICSS_20220624_griddedPop.pptx contains the presentation.\r\nData used\r\nFor that purpose, we will need to access three data sources:\r\nPopulation data from the Bottom-up gridded population estimates for Nigeria, version 2.0, produced jointly by WorldPop and the National Population Commission of Nigeria and accessible here,\r\nHealth facilities locations produced by GRID3 Nigeria and accessible here,\r\nLocal Government Area operational boundaries released by GRID3 Nigeria and accessible here On github the three data file are stored for this tutorial.\r\n## Practical\r\n\r\n\r\nlibrary(tidyverse) #R library to manipulate table data\r\nlibrary(tmap) # R library to plot interactive maps\r\nlibrary(kableExtra) # R library for nice tables\r\n\r\ntmap_mode('view') # set tmap as interactive\r\ntmap_options(check.and.fix = TRUE) \r\n\r\n\r\n\r\nDefining the study area ————————————————-\r\n\r\n\r\nlibrary(sf) # R library to manipulate spatial data\r\n\r\n\r\n\r\nWe load the vector file of administrative regions in Nigeria\r\n\r\n\r\nlga <- st_read(\r\n  paste0(\r\n    data_path,\r\n    '/GRID3_Nigeria_-_Local_Government_Area_Boundaries/GRID3_Nigeria_-_Local_Government_Area_Boundaries.shp'))\r\n\r\n\r\nReading layer `GRID3_Nigeria_-_Local_Government_Area_Boundaries' from data source `C:\\Users\\ecd1u18\\Documents\\SICSS-covenant-gridded-population\\data\\GRID3_Nigeria_-_Local_Government_Area_Boundaries\\GRID3_Nigeria_-_Local_Government_Area_Boundaries.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 774 features and 7 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2.692613 ymin: 4.270204 xmax: 14.67797 ymax: 13.88571\r\nGeodetic CRS:  WGS 84\r\n\r\nlga %>% \r\n      glimpse()\r\n\r\n\r\nRows: 774\r\nColumns: 8\r\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15~\r\n$ lga_code   <chr> \"10001\", \"10002\", \"10003\", \"10004\", \"10005\", \"100~\r\n$ state_code <chr> \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"~\r\n$ lga_name_x <chr> \"Aniocha North\", \"Aniocha South\", \"Bomadi\", \"Buru~\r\n$ mean       <dbl> 91927.45, 193913.27, 32604.42, 53159.27, 153016.5~\r\n$ Shape__Are <dbl> 0.033182160, 0.070862318, 0.016922270, 0.14636025~\r\n$ Shape__Len <dbl> 0.8457545, 1.2001197, 1.0004980, 2.4157890, 1.068~\r\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((6.438156 6...., MULT~\r\n\r\nWe plot using tmap interactive functions\r\n\r\n\r\ntm_shape(lga)+\r\n  tm_polygons()\r\n\r\n\r\n\r\n\r\nWe will focus our analysis in the local government area of Ado Odo/Ota\r\n\r\n\r\nlga_Ado <- lga %>% \r\n  filter(lga_name_x=='Ado Odo/Ota')\r\n\r\n\r\ntm_shape(lga_Ado)+\r\n  tm_borders(col='orange', lwd=5)+\r\n  tm_shape(lga)+\r\n  tm_borders()+\r\n  tm_basemap('OpenStreetMap')\r\n\r\n\r\n\r\n\r\nDiscovering the health facilities dataset ——————————-\r\n\r\n\r\nhealth_facilities <- st_read(\r\n  paste0(\r\n    data_path, \r\n    '/GRID3_Nigeria_-_Health_Care_Facilities_/GRID3_Nigeria_-_Health_Care_Facilities_.shp'))\r\n\r\n\r\nReading layer `GRID3_Nigeria_-_Health_Care_Facilities_' from data source `C:\\Users\\ecd1u18\\Documents\\SICSS-covenant-gridded-population\\data\\GRID3_Nigeria_-_Health_Care_Facilities_\\GRID3_Nigeria_-_Health_Care_Facilities_.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 46146 features and 20 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: 2.70779 ymin: 4.281802 xmax: 14.63638 ymax: 13.86524\r\nGeodetic CRS:  WGS 84\r\n\r\nhealth_facilities %>% \r\n  st_drop_geometry() %>% \r\n  glimpse()\r\n\r\n\r\nRows: 46,146\r\nColumns: 20\r\n$ FID        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15~\r\n$ editor     <chr> \"tosin.williams\", \"mokobia.chidinma\", \"tosin.will~\r\n$ timestamp  <date> 2020-07-04, 2020-07-04, 2020-07-04, 2020-07-04, ~\r\n$ lga_code   <int> 124, 124, 124, 124, 124, 124, 124, 124, 124, 124,~\r\n$ lga_name   <chr> \"Maiduguri\", \"Maiduguri\", \"Maiduguri\", \"Maiduguri~\r\n$ ward_name  <chr> \"Maisandari\", \"Maisandari\", \"Maisandari\", \"Maisan~\r\n$ ward_code  <chr> \"12413\", \"12413\", \"12413\", \"12413\", \"12413\", \"124~\r\n$ longitude  <dbl> 13.14832, 13.14718, 13.14227, 13.14134, 13.11913,~\r\n$ latitude   <dbl> 11.82232, 11.82161, 11.79810, 11.81027, 11.83188,~\r\n$ updated_on <chr> \"2019-03-01\", \"2019-03-01\", \"2019-03-01\", \"2019-0~\r\n$ accessibil <chr> NA, \"Unknown\", NA, NA, NA, \"Unknown\", \"Unknown\", ~\r\n$ functional <chr> \"Unknown\", \"Functional\", \"Functional\", \"Functiona~\r\n$ category   <chr> \"Primary Health Center\", \"Primary Health Center\",~\r\n$ ownership  <chr> \"Private\", \"National Primary Healthcare Developme~\r\n$ type       <chr> \"Primary\", \"Primary\", \"Primary\", \"Primary\", \"Prim~\r\n$ source     <chr> \"eHA Polio\", \"Measles Campaign\", \"eHA Polio\", \"He~\r\n$ alternate_ <chr> \"Nursing Home\", \"Nursing Home\", \"Atla Medical Cen~\r\n$ primary_na <chr> \"G R A Nursing Home\", \"Gishili Health Center\", \"G~\r\n$ globalid   <chr> \"af719462-abfd-4f47-9dc3-0987164e75ac\", \"a29b0328~\r\n$ state_code <chr> \"BR\", \"BR\", \"BR\", \"BR\", \"BR\", \"BR\", \"BR\", \"BR\", \"~\r\n\r\nhealth_facilities_Ado <- health_facilities %>% \r\n  filter(lga_name=='Ado Odo/Ota')\r\n\r\n\r\n\r\nEXERCISE: How many health facilities in Ado Odo/Ota?\r\n\r\nClick for the solution\r\n\r\n\r\ndim(health_facilities_Ado)\r\n\r\n\r\n[1] 216  21\r\n\r\nWe now represent the health facilities in Ado by facility types. We specify than on hoover the name of the facility primary_na is shown and on click the name, primary_na, the function status, functional and the source of the information, source.\r\n\r\n\r\ntm_shape(health_facilities_Ado)+\r\n  tm_dots(col='type', size=0.07, id='primary_na', \r\n          popup.vars=c('category','functional','source'))+\r\n  tm_basemap('OpenStreetMap')+\r\n  tm_shape(lga_Ado)+\r\n  tm_borders(lwd=4)\r\n\r\n\r\n\r\n\r\nEXERCISE: How many health facilities are offering tertiary services in Ado Odo/Ota?\r\n\r\nClick for the solution\r\n\r\n\r\ntable(health_facilities_Ado$type)\r\n\r\n\r\n\r\n  Primary Secondary  Tertiary \r\n      205         6         5 \r\n\r\nDiscovering the gridded population dataset ——————————\r\n\r\n\r\nlibrary(raster) # R library to manipulate raster data\r\n\r\npop <- raster(\r\n  paste0(\r\n    data_path, \r\n    '/NGA_population_v2_0_gridded/NGA_population_v2_0_gridded.tif'))\r\n\r\n\r\n\r\nFirst spatial manipulation: cropping (or cutting a raster file to the extent of another spatial file, here the extent or bounding box of the Ado LGA)\r\n\r\n\r\npop_Ado <- crop(pop, lga_Ado)\r\nplot(pop_Ado)\r\n\r\n\r\n\r\n\r\nSecond spatial manipulation: masking (or modifying the values of raster grid cells designated by a second raster, here the values outside of the boundaries of the Ado LGA )\r\n\r\n\r\npop_Ado <- mask(pop_Ado, lga_Ado)\r\nplot(pop_Ado)\r\n\r\n\r\n\r\n\r\nAnd now we combine health facilities informaiton with gridded population\r\n\r\n\r\ntm_shape(health_facilities_Ado)+\r\n  tm_dots(col='type', size=0.07, id='primary_na', \r\n          popup.vars=c('category','functional','source'))+\r\n  tm_basemap('OpenStreetMap')+\r\n  tm_shape(pop_Ado)+\r\n  tm_raster()+\r\n  tm_shape(lga_Ado)+\r\n  tm_borders(lwd=4)\r\n\r\n\r\n\r\n\r\nBuffering points ——————————————————–\r\nWe want to know how many people are living around each of the health facilities, that leads us to our third spatial manipulation: buffering or drawing a circle of a giving radius around each point.\r\n\r\n\r\nlibrary(units) # R library to work with unit systems\r\n\r\nhealth_facilities_Ado_buffered <- st_buffer(health_facilities_Ado, dist=set_units(1, km))\r\n\r\n\r\n\r\nWe visualise the first observation, that is about the Ado Odo Ii Health Center\r\n\r\n\r\nhealth_facilities_Ado[1,]\r\n\r\n\r\nSimple feature collection with 1 feature and 20 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: 2.937438 ymin: 6.592648 xmax: 2.937438 ymax: 6.592648\r\nGeodetic CRS:  WGS 84\r\n   FID           editor  timestamp lga_code    lga_name ward_name\r\n1 2835 mokobia.chidinma 2018-12-18    28003 Ado Odo/Ota     Ado 2\r\n  ward_code longitude latitude updated_on accessibil functional\r\n1  OGSADO02  2.937438 6.592648 2019-03-01       <NA> Functional\r\n               category             ownership    type source\r\n1 Primary Health Center Local Government Area Primary   GRID\r\n  alternate_               primary_na\r\n1       <NA> Ado Odo Ii Health Center\r\n                              globalid state_code\r\n1 5e82364d-25dc-4e34-95da-9ea0dbf688ab         OG\r\n                   geometry\r\n1 POINT (2.937438 6.592648)\r\n\r\ntm_shape(health_facilities_Ado_buffered[1,])+\r\n  tm_borders()+\r\n  tm_shape(pop_Ado)+\r\n  tm_raster()+\r\n  tm_shape(health_facilities_Ado[1,])+\r\n  tm_dots( size=0.08, id='primary_na', popup.vars=c('category','functional','source'))+\r\n  tm_basemap('OpenStreetMap')\r\n\r\n\r\n\r\n\r\nComputing the population ————————————————\r\nWe now want to compute the number of people living in 1 km of each facility by aggregating the related grid cells.\r\n\r\n\r\nhealth_facilities_Ado_pop <- raster::extract(pop_Ado, health_facilities_Ado_buffered, \r\n                                             fun=sum, na.rm=T,df=T)\r\n\r\nkbl(head(health_facilities_Ado_pop)) %>% \r\n  kable_minimal()\r\n\r\n\r\n\r\nID\r\n\r\n\r\nNGA_population_v2_0_gridded\r\n\r\n\r\n1\r\n\r\n\r\n12037.952\r\n\r\n\r\n2\r\n\r\n\r\n6758.998\r\n\r\n\r\n3\r\n\r\n\r\n7803.548\r\n\r\n\r\n4\r\n\r\n\r\n13044.511\r\n\r\n\r\n5\r\n\r\n\r\n6647.654\r\n\r\n\r\n6\r\n\r\n\r\n3586.548\r\n\r\n\r\nWe see floating point number: this is due to gridded population that has floating point number in its grid cells, as a result of a probabilistic model. It is the most likely value as an average of all the likely values of people count in the grid cell. For more information: WorldPop. 2019. Bottom-up gridded population estimates for Nigeria, version 1.2. WorldPop, University of Southampton. doi:10.5258/SOTON/WP00655.\r\n\r\n\r\nhealth_facilities_Ado_buffered$pop <- health_facilities_Ado_pop$NGA_population_v2_0_gridded\r\n\r\n\r\n\r\nEXERCISE: How many people are living in 1km of Ado Odo Ii Health Center?\r\n\r\nClick for the solution\r\n\r\n\r\nhealth_facilities_Ado_buffered[1,]\r\n\r\n\r\nSimple feature collection with 1 feature and 21 fields\r\nGeometry type: POLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2.928364 ymin: 6.583597 xmax: 2.946551 ymax: 6.60169\r\nGeodetic CRS:  WGS 84\r\n   FID           editor  timestamp lga_code    lga_name ward_name\r\n1 2835 mokobia.chidinma 2018-12-18    28003 Ado Odo/Ota     Ado 2\r\n  ward_code longitude latitude updated_on accessibil functional\r\n1  OGSADO02  2.937438 6.592648 2019-03-01       <NA> Functional\r\n               category             ownership    type source\r\n1 Primary Health Center Local Government Area Primary   GRID\r\n  alternate_               primary_na\r\n1       <NA> Ado Odo Ii Health Center\r\n                              globalid state_code\r\n1 5e82364d-25dc-4e34-95da-9ea0dbf688ab         OG\r\n                        geometry      pop\r\n1 POLYGON ((2.937652 6.601684... 12037.95\r\n\r\nLet’s look at the distribution of the number of people living at 1km of health facility in Ado.\r\n\r\n\r\nsummary(health_facilities_Ado_buffered$pop)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n    141   11887   19859   18951   27599   34111 \r\n\r\nhist(health_facilities_Ado_buffered$pop, breaks=20)\r\n\r\n\r\n\r\ntm_shape(health_facilities_Ado_buffered)+\r\n  tm_fill('pop', style='pretty', id='pop')+\r\n  tm_shape(health_facilities_Ado)+\r\n  tm_dots( size=0.08, id='primary_na', popup.vars=c('category','functional','source'))+\r\n  tm_basemap('OpenStreetMap')\r\n\r\n\r\n\r\n\r\nWe can highlight in the less populated area\r\n\r\n\r\ntm_shape(health_facilities_Ado_buffered %>% \r\n           filter(pop<1000))+\r\n  tm_fill( id='pop', alpha=0.5, col='grey20')+\r\n  tm_shape(pop_Ado)+\r\n  tm_raster()+\r\n  tm_basemap('OpenStreetMap')\r\n\r\n\r\n\r\n\r\nHow many people are not covered by health facilities? ——————\r\nWe want now to compute the number of people that are not at 1km of a health facility We first need to compute the total number of people at 1km from a health facility. We can’t just sum up the previous column as some people are at less than a 1km of several health facilities such that they will be counted several times.\r\nWe need a fourth spatial manipulation: rasterizing. We need to convert the buffers into a raster\r\n\r\n\r\nhealth_facilities_Ado_buffered_rasterized <- rasterize(health_facilities_Ado_buffered,\r\n                                                       pop_Ado, field=1)\r\nplot(health_facilities_Ado_buffered_rasterized)\r\n\r\n\r\n\r\n\r\nThen we mask the gridded population with the rasterized buffer, which gives us the population covered by the health facilities\r\n\r\n\r\npop_Ado_masked <- mask(pop_Ado, health_facilities_Ado_buffered_rasterized)\r\nplot(pop_Ado_masked)\r\n\r\n\r\n\r\nsum(pop_Ado_masked[], na.rm=T)\r\n\r\n\r\n[1] 1142942\r\n\r\nEXERCISE: How many people are not living in a 1km of an health facility? For that purpose we need to compute the number of people living in Ado.\r\n\r\nClick for the solution\r\nfirst method\r\n\r\n\r\nsum(pop_Ado[], na.rm=T) - sum(pop_Ado_masked[], na.rm=T)\r\n\r\n\r\n[1] 672583.2\r\n\r\nsecond method\r\n\r\n\r\nlga_Ado_pop <- raster::extract(pop, lga_Ado, fun=sum, na.rm=T,df=T)\r\nsum(lga_Ado_pop)  - sum(pop_Ado_masked[], na.rm=T)\r\n\r\n\r\n[1] 672584.2\r\n\r\nthird method\r\n\r\n\r\nlga_Ado$mean  - sum(pop_Ado_masked[], na.rm=T)\r\n\r\n\r\n[1] 672583.2\r\n\r\nHow many are not covered by a maternity home? —————————\r\nLet’s focused on my health facility type: the maternity homes.\r\n\r\n\r\ntm_shape(pop_Ado)+\r\n  tm_raster()+\r\n  tm_shape(health_facilities_Ado)+\r\n  tm_dots( size=0.08, id='primary_na', \r\n           popup.vars=c('category','functional','source'))+\r\n  tm_shape(health_facilities_Ado %>% \r\n             filter(category=='Maternity Home'))+\r\n  tm_dots(col='darkgreen', size=0.08, id='primary_na', \r\n          popup.vars=c('category','functional','source'))+\r\n  tm_basemap('OpenStreetMap')\r\n\r\n\r\n\r\n\r\nEXERCISE: How many maternity homes are listed in the LGA?\r\n\r\nClick for the solution\r\n\r\n\r\nhealth_facilities_Ado_buffered_maternity <- health_facilities_Ado_buffered %>% \r\n  filter(category=='Maternity Home')\r\nnrow(health_facilities_Ado_buffered_maternity)\r\n\r\n\r\n[1] 69\r\n\r\nEXERCISE: How many people are not living in a 1km distance of a maternity center?\r\n\r\nClick for the solution\r\n\r\n\r\nhealth_facilities_Ado_buffered_maternity_rasterized <- rasterize(\r\n  health_facilities_Ado_buffered_maternity, pop_Ado, field=1\r\n)\r\nplot(health_facilities_Ado_buffered_maternity_rasterized)\r\n\r\n\r\n\r\npop_Ado_masked_maternity <- mask(pop_Ado, \r\n                                 health_facilities_Ado_buffered_maternity_rasterized)\r\nplot(pop_Ado_masked_maternity)\r\n\r\n\r\n\r\nsum(pop_Ado[], na.rm=T) - sum(pop_Ado_masked_maternity[], na.rm=T)\r\n\r\n\r\n[1] 1149942\r\n\r\nWhat is the furthest a woman has to travel to reach a maternity? ——–\r\nWe first subset the maternity homes\r\n\r\n\r\nhealth_facilities_Ado_maternity <- health_facilities_Ado %>% \r\n  filter(category=='Maternity Home')\r\n\r\n\r\n\r\nFifth spatial manipulation: We compute a gridded distance that quantifies the Euclidean distance from each of the grid cells to the nearest maternity\r\n\r\n\r\nhealth_facilities_Ado_maternity_distance <- distanceFromPoints(pop_Ado, \r\n                                                               health_facilities_Ado_maternity)\r\nplot(health_facilities_Ado_maternity_distance)\r\n\r\n\r\n\r\n\r\nWe then focused only on the grid cells that are populated\r\n\r\n\r\nhealth_facilities_Ado_maternity_distance_pop <- mask(health_facilities_Ado_maternity_distance, \r\n                                                     pop_Ado)\r\nplot(health_facilities_Ado_maternity_distance_pop)\r\n\r\n\r\n\r\n\r\nWe can now have a look at the distribution of the distance to maternity in the LGA.\r\n\r\n\r\nsummary(health_facilities_Ado_maternity_distance_pop[])\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n  10.68  983.01 1916.11 2369.69 3207.52 9604.39  117769 \r\n\r\nEXERCISE: What is the furthest people are leaving from a maternity in Ado?\r\n\r\nClick for the solution\r\n\r\n\r\nmax(health_facilities_Ado_maternity_distance_pop[], na.rm=T)\r\n\r\n\r\n[1] 9604.385\r\n\r\nEXERCISE: How many people are living at more than 8km from a maternity? This exercise is based on all the previous analyses steps\r\n\r\nClick for the solution\r\n\r\n#We compute a buffer of 8 km (rather than 1km)\r\nhealth_facilities_Ado_maternity_buffered8km <- st_buffer(health_facilities_Ado_maternity,\r\n                                                         dist=set_units(8, km))\r\n\r\n#We subtract the population\r\nhealth_facilities_Ado_maternity_buffered8km_rasterized <- rasterize(\r\n  health_facilities_Ado_maternity_buffered8km, pop_Ado, field=1)\r\nplot(health_facilities_Ado_maternity_buffered8km_rasterized)\r\n\r\n\r\n\r\npop_Ado_masked_maternity_8km <- mask(pop_Ado, \r\n                                     health_facilities_Ado_maternity_buffered8km_rasterized, \r\n                                     inverse=T)\r\nplot(pop_Ado_masked_maternity_8km)\r\n\r\n\r\n\r\n# We sum up the population\r\nsum(pop_Ado_masked_maternity_8km[], na.rm=T)\r\n\r\n\r\n[1] 9981.121\r\n",
    "preview": "posts/2022-06-24-sicss-covenantusing-gridded-population-to-gain-spatial-insights-in-r/./pic/final_map.png",
    "last_modified": "2024-03-20T11:41:06+00:00",
    "input_file": {},
    "preview_width": 857,
    "preview_height": 555
  },
  {
    "path": "posts/2021-11-15-statistical-population-modelling-for-census-support/",
    "title": "Statistical population modelling for census support",
    "description": "I led a one-week workshop based on a series of hands-on tutorial to grasp the bases of the Bayesian approach for \npopulation models. All the materials, including Stan code, written tutos and presentation are\navailable online on a dedicated website. It was first delivered to the Brazilian Stats Office (IBGE).",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "academia",
      "tutorial"
    ],
    "contents": "\r\nStatistical population modelling is a powerful tool for producing gridded population estimates\r\nto support census activities.\r\nWorldPop at the University of Southampton is a global leader in developing these methods and\r\nhas partnered with the United Nations Population Fund (UNFPA) to\r\nprovide support to national statistics offices in training and production of\r\nhigh-resolution gridded population estimates from existing data sources\r\n(e.g. household surveys, building footprints, administrative records, census projections).\r\nThis website provides a series of tutorials in Bayesian statistics for population modelling and hands-on experience to start developing the necessary skills. It includes example code and other resources designed to expedite the learning curve as much as possible.\r\nIt includes example code and other resources designed to expedite the learning curve as much as possible.\r\nThe key concepts that are covered in the tutorial series include:\r\nIntroduction to software for Bayesian statistical modelling: R and Stan,\r\nSimple linear regression in a Bayesian context,\r\nRandom effects to account for settlement type (e.g. urban/rural) and other types of stratification in survey data,\r\nQuantifying and mapping uncertainties in population estimates and\r\nDiagnostics to evaluate model performance (e.g. cross-validation).\r\nThe material has been used during a remote workshop with the Brazilian Stats Office, Instituto Brasileiro de Geografia e Estatística (IBGE), in October 2021.\r\nMaterial\r\nIntroduction\r\nTutorial 1: How to think like a Bayesian and build a first population model? Quiz\r\nTutorial 2: How to model large-scale spatial variations? Quiz\r\nTutorial 3: How to model small-scale spatial variations? Quiz\r\nTutorial 4: Advanced model diagnostics and prediction\r\nConclusion\r\nRaw code\r\nThe raw code of the website and tutorials, including the R code can be found here.\r\nAcknowledgements\r\nThis tutorial was written by Edith Darin from WorldPop, University of Southampton and Douglas Leasure from Leverhulme Centre for Demographic Science, University of Oxford, with supervision from Andrew Tatem, WorldPop, University of Southampton. Funding for the work was provided by the United Nations Population Fund (UNFPA).\r\nLicense\r\nYou are free to redistribute this document under the terms of a Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license.\r\nSuggested citation\r\nDarin E, Leasure DR, Tatem AJ. 2021. Statistical population modelling for census support. WorldPop, University of Southampton, https://wpgp.github.io/bottom-up-tutorial/, doi:10.5281/zenodo.5572490\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-15-statistical-population-modelling-for-census-support/cover2.PNG",
    "last_modified": "2025-09-10T17:04:18+01:00",
    "input_file": {},
    "preview_width": 1693,
    "preview_height": 685
  },
  {
    "path": "posts/2021-11-09-efficient-computation-across-multiple-rasters/",
    "title": "Efficient computation across multiple rasters",
    "description": "One day you want to compute one quantile across 95 rasters, and suddenly your script takes 7 hours to run, luckily at night. \nAnd then you're facing the truth: you will have to dig into the stars R-package. But to this date, few implementations lie around.\nSo we share here one demonstration of the efficiency of stars with a special big up for the st_apply function.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2021-11-09",
    "categories": [],
    "contents": "\r\nstars R package (Pebesma 2021) has been for me for a while the unreachable R fruit, almost a Tantalus-like temptation.\r\n\r\n\r\n\r\nAnd then I needed to compute the 95th percentile of a stack of 95 rasters. I guessed stars could be a solution but my particular task - that is summarising at grid cell level a raster stack - was not directly covered in the manual. And stars is still young and does not benefit of a long stash of StackOverflow questions (1000+ for the r-rastertag)\r\nTherefore I will share the simple workflow of computing a quantile across a stack of raster in the stars framework. I will cover the stars notions of: dimension, stars_proxy and st_apply.\r\nYou first load the rasters in a stack named y:\r\n\r\n\r\nlibrary(stars)\r\nls <- list.files(dir,  full.names=TRUE)\r\ny <-  read_stars(ls, quiet=T, proxy =T, along='attributes')\r\n\r\n\r\n\r\nHere occur our first two tricks.\r\nFirst we read the rasters in a stars_proxy object which doesn’t load the rasters in memory.\r\nSecond, we tell stars to create one object that stacks all the rasters, in a dimension called attributes, such that y is a stars object with three dimensions: x, y, attributes.\r\n\r\nThen we compute our grid-cell metric by using st_apply across this third dimension.\r\n\r\n\r\nq_95 <- st_apply(y, 1:2, function(x)  quantile(x, probs=0.95, na.rm=TRUE), \r\n              PROGRESS = T,\r\n              FUTURE = T)\r\n\r\n\r\n\r\nTwo last tricks:\r\nPROGRESS =T shows a very informative progress bar\r\n\r\nFUTURE = T makes use of the parallel processing offered by the future.apply R package.\r\nLastly, in order to actually run the computation, we need to tell R to save the output such that it will starts the process:\r\n\r\n\r\nwrite_stars(q_95, outdir)\r\n\r\n\r\n\r\nResults: the computation took 2h instead of 7h30.\r\n\r\n\r\n\r\nPebesma, Edzer. 2021. Stars: Spatiotemporal Arrays, Raster and Vector Data Cubes. https://CRAN.R-project.org/package=stars.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-09-efficient-computation-across-multiple-rasters/./tantalus_cube.png",
    "last_modified": "2024-03-20T11:41:06+00:00",
    "input_file": {},
    "preview_width": 474,
    "preview_height": 627
  },
  {
    "path": "posts/2021-10-21-which-population-microdata-can-we-openly-access/",
    "title": "Which population microdata are openly accessible?",
    "description": "Population microdata are key input to estimate and map population. \nThey are however often difficult to access when derived from household survey pre-listings: (1) because of the limited number of\nhousehold survey (2) because of the confidentiality of cluster locations (3) because \nit is operationnal data that is not cleaned and stored appropriately for reuse.\nWe explore in this blog post the availibility of two promising datasets for population modelling:\nIPUMS-International and the Demographic and Health Surveys programm.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2021-10-21",
    "categories": [],
    "contents": "\r\nPopulation microdata are key input to estimate and map population in the absence of a complete and recent census (Wardrop et al. 2018). D. R. Leasure et al. (2020) developed a hierarchical Bayesian modelling framework combining pre-survey household listing with geospatial covariates to predict population count with high spatial resolution.\r\nIn settings where household listings are not available, Boo et al. (2019) carried out a microcensus survey, that is a household survey in well-defined small clusters aiming at enumerating the resident population.\r\nHousehold surveys are expensive exercises and operational data such as household pre-listings are often not accessible because not deemed worth cleaning, pseudonymising, or even storing.\r\nTo overcome those limitations in accessing population microdata, D. Leasure, Tatem, and Bondarenko (2020) developed a Bayesian model with the Integrated Public Use Microdata Series-International (Ruggles et al. 2003), that is a 10% sample of individual data from censuses, combined with building footprints (Ecopia.AI and Maxar Technologies 2019) and national projections (Raftery, Alkema, and Gerland 2014).\r\nThere is another source for population microdata that is available upon simple request: the Demographic and Health Surveys (Corsi et al. 2012).\r\nIn this blog post, we explore the availability of both data sets across time and country.\r\nWe copy/pasted available datasets from the download website page of both institutions as well as a table linking countries to their continent and we stored them here:\r\nIf you want to access the code just fold out this block:\r\n\r\n\r\nhide\r\n\r\nlibrary(tidyverse)\r\nlibrary(ggplot2)\r\nlibrary(kableExtra)\r\n\r\ncontinent <- read_csv('data/continent_country.csv')\r\nipums <- read_csv( 'data/ipums_samples_date.csv')\r\ndhs <- read_csv('data/dhs_samples_date.csv')\r\n\r\n# DHS metadata cleaning\r\n\r\ndhs_cleaned <- dhs %>% \r\n  rowwise() %>% \r\n  mutate(\r\n    Survey =  str_remove(Survey, \" \\\\(.+\\\\)\"),\r\n    country = str_remove(Survey, \" \\\\d+\\\\-*\\\\d*+\"),\r\n    year = str_split(Survey, \" \")[[1]][length(str_split(Survey, \" \")[[1]])],\r\n    year = ifelse(grepl(\"-\", year), paste0(str_sub(year,1,2), str_sub(year, -2,-1)), year),\r\n    source = 'dhs'\r\n  ) %>% \r\n  select(-Survey, -`GPS Datasets`)\r\n\r\n# IPUMS metadata formatting\r\nipums_cleaned <- ipums %>% \r\n      pivot_longer(-country, values_to = 'year') %>% \r\n      select(-name) %>% \r\n      filter(!is.na(year)) %>% \r\n      mutate(source = 'ipums')\r\n\r\n# Build master dataset for visualising\r\nmaster <- rbind( ipums_cleaned, dhs_cleaned) %>% \r\n  ungroup() %>% \r\n  # assign continent to countries\r\n  left_join(\r\n    continent\r\n  ) %>% \r\n  # prepare variable for plotting\r\n  mutate(\r\n    country_f = fct_reorder(country, continent),\r\n    year_label= paste0(str_sub(year, 1,3), \"0\")) %>% \r\n  filter(!is.na(continent)& year_label>=1960) \r\n\r\n# Plot data\r\n\r\nyear_label <- unique(master$year_label)\r\n\r\nggplot(master , aes(x=year, y=country_f,  color=source))+\r\n  geom_line()+\r\n  geom_point()+\r\n  # replace individual year by grouped year\r\n  scale_x_discrete(breaks=year_label, labels=year_label)+\r\n  # remove coutnry names\r\n  scale_y_discrete(breaks=NULL, name='')+\r\n  # use faceting to group country by continent\r\n  facet_grid(rows = vars(continent), scales = \"free_y\", switch='y', space = \"free_y\") +\r\n  theme_minimal()+\r\n  theme(panel.spacing = unit(0, \"lines\"), \r\n        strip.background = element_blank(),\r\n        strip.placement = \"outside\",\r\n        strip.text.y.left = element_text(angle = 0))+\r\n  labs(title= 'Dates of available DHS and IPUMS data for every country')+\r\n  scale_colour_manual(values=c( \"seagreen4\", \"orchid1\"))\r\n\r\n\r\n\r\n\r\nLow and middle income countries are concentrated in Africa, Asia and South America, where we see less data avaialble through IPUMS and more regular data from DHS.\r\n\r\n\r\n\r\nBoo, Gianluca, Édith Darin, Douglas R. Leasure, and Andrew Tatem. 2019. “High-Resolution Population Mapping and Estimation in the Western Part of the Democratic Republic of Congo.” Unpublished. https://doi.org/10.13140/RG.2.2.24975.94880.\r\n\r\n\r\nCorsi, Daniel J, Melissa Neuman, Jocelyn E Finlay, and SV Subramanian. 2012. “Demographic and Health Surveys: A Profile.” International Journal of Epidemiology 41 (6): 1602–13. https://doi.org/10.1093/ije/dys184.\r\n\r\n\r\nEcopia.AI, and Maxar Technologies. 2019. Digitize Africa Data. http://digitizeafrica.ai.\r\n\r\n\r\nLeasure, Douglas R, Warren C Jochem, Eric M Weber, Vincent Seaman, and Andrew J Tatem. 2020. “National Population Mapping from Sparse Survey Data: A Hierarchical Bayesian Modeling Framework to Account for Uncertainty.” Proceedings of the National Academy of Sciences.\r\n\r\n\r\nLeasure, Douglas, Andrew Tatem, and Maksym Bondarenko. 2020. “A Bayesian Approach to Produce 100 m Gridded Population Estimates Using Census Microdata and Recent Building Footprints.”\r\n\r\n\r\nRaftery, Adrian E., Leontine Alkema, and Patrick Gerland. 2014. “Bayesian Population Projections for the United Nations.” Statistical Science : A Review Journal of the Institute of Mathematical Statistics 29 (1): 58–68. https://doi.org/10.1214/13-STS419.\r\n\r\n\r\nRuggles, Steven, Miriam L. King, Deborah Levison, Robert McCaa, and Matthew Sobek. 2003. “IPUMS-International.” Historical Methods: A Journal of Quantitative and Interdisciplinary History 36 (2): 60–65.\r\n\r\n\r\nWardrop, NA, WC Jochem, TJ Bird, HR Chamberlain, D Clarke, D Kerr, L Bengtsson, S Juran, V Seaman, and AJ Tatem. 2018. “Spatially Disaggregated Population Estimates in the Absence of National Population and Housing Census Data.” Proceedings of the National Academy of Sciences 115 (14): 35293537. https://doi.org/10.1073/pnas.1715305115.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-21-which-population-microdata-can-we-openly-access/./dates_plot.png",
    "last_modified": "2024-03-20T11:41:06+00:00",
    "input_file": {},
    "preview_width": 770,
    "preview_height": 475
  },
  {
    "path": "posts/2021-07-31-unraveling-hierarchichal-data-structure-with-sunburst-charts/",
    "title": "How to get a clear view of hierarchical data? Answer: Sunburst Charts",
    "description": "Sunburst charts are a great tool to visualise hierarchichal data structure and unravel missing combinations. Useful as an exploratory analysis for Bayesian hierarchical models.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2021-07-31",
    "categories": [],
    "contents": "\r\nBackground\r\nBayesian models are great for hierarchical data, that is observations that have a grouped structure, typically grades of students that belongs to different classrooms in different schools.\r\nThis type of data structure violates the assumption of independence, key for basic models and can produce mislead conclusions, when the group-specific patterns differ from the global pattern.\r\nBut the point here is not to describe Bayesian hierarchichal models.\r\nWe are taking a step backward and we want to visualise the hierarchical structure of our data, for two reasons:\r\ncommunicate with a lay audience\r\nexplore the groupings\r\nIndeed there are two types of data hierarchy: crossed and nested. A crossed design happens when all combination of groupings exist (either in data or in reality). Typically if we were to distinguish between foreign-born and native students, we would expect that this grouping can occur in every classroom. On the opposite, a nested design describes a strict hierarchy, typically, one classroom belongs to one and only one school.\r\nWe want to show that sunburst charts1 are a great tool to visualise the hierarchy and combined with some tidyverse magic can unravel missing combinations.\r\n\r\nR set-up\r\n\r\n\r\nshow\r\n\r\nlibrary(tidyverse)\r\nlibrary(RColorBrewer)\r\nlibrary(kableExtra)\r\nlibrary(plotly) # contains function for sunburst plot\r\n\r\n\r\n\r\nData\r\nWe will use a dataset provided by Leasure and al. as supplement to their paper (Douglas R. Leasure et al. 2020a).\r\n\r\n\r\nshow\r\n\r\n# download  tutorial data\r\ndownload.file(\r\n  \"https://www.pnas.org/highwire/filestream/949050/field_highwire_adjunct_files/1/pnas.1913050117.sd01.xls\",\r\n  '_posts/2021-07-31-unraveling-hierarchichal-data-structure-with-sunburst-charts/nga_demo_data.xls',\r\n  method='libcurl',\r\n  mode='wb'\r\n)\r\ngetwd()\r\n\r\n\r\n\r\nIt consist in household surveys that collected information on the total population in 1141 clusters in 15 of 37 states in Nigeria during 2016 and 2017. The surveys are further described in Douglas R. Leasure et al. (2020b) and Weber et al. (2018).\r\n\r\n\r\nshow\r\n\r\n# prepare data\r\ndata <- readxl::read_excel('_posts/2021-07-31-unraveling-hierarchichal-data-structure-with-sunburst-charts/nga_demo_data.xls')\r\n\r\ndata <- data %>% \r\n  mutate(\r\n    id = paste0('cluster',1:nrow(data)),\r\n    pop_density=N/A\r\n  ) %>% \r\n  rename(\r\n    population=N\r\n  ) %>% \r\n  select(id, population, pop_density, type, region, state, local )\r\n\r\ndata %>% head()  %>% kbl() %>% kable_minimal()\r\n\r\n\r\n\r\nid\r\n\r\n\r\npopulation\r\n\r\n\r\npop_density\r\n\r\n\r\ntype\r\n\r\n\r\nregion\r\n\r\n\r\nstate\r\n\r\n\r\nlocal\r\n\r\n\r\ncluster1\r\n\r\n\r\n547\r\n\r\n\r\n180.11207\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\ncluster2\r\n\r\n\r\n803\r\n\r\n\r\n268.57792\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n10\r\n\r\n\r\ncluster3\r\n\r\n\r\n750\r\n\r\n\r\n243.64275\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\ncluster4\r\n\r\n\r\n281\r\n\r\n\r\n93.06772\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\ncluster5\r\n\r\n\r\n730\r\n\r\n\r\n241.30606\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\ncluster6\r\n\r\n\r\n529\r\n\r\n\r\n171.19344\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\nThere are two sources of grouping in the data: by settlement type and by administrative divisions. There is three levels of administrative divisions reported: region, state and local governement area local. Settlement type is characteristic of a crossed hierarchy: every settlement type can be (potentially) present in all subgrouping eg in all regions, states or local government area.\r\nWe plot first the variation of the response variable population_density by settlement type\r\n\r\n\r\nshow\r\n\r\n# plot population density per region\r\nggplot(data %>% \r\n         group_by(\r\n           region\r\n         ) %>% \r\n         mutate(\r\n           mean_popDens = mean(pop_density)\r\n           ) %>% \r\n         ungroup(), \r\n       aes(fill=mean_popDens, x=pop_density, y=as.factor(region)))+\r\n  geom_boxplot()+\r\n  theme_minimal()+\r\n  scale_fill_stepsn( colours = brewer.pal(6, \"YlOrRd\"))+\r\n  labs(fill='Mean \\npopulation \\ndensity', x='Population density', y='Region')\r\n\r\n\r\n\r\n\r\nFigure 1: Boxplot of population densities per region\r\n\r\n\r\n\r\nFigure 1 shows the motivation for building a hierarchical model.\r\nVisualising the hierarchical structure\r\nWe will make the use of the chart type sunburstof the package plotly.\r\nWe need first to prepare the data with unique identifier for each administrative region:\r\n\r\n\r\nshow\r\n\r\n# create unique id for the nested admin level\r\ndata <- data %>% \r\n  mutate(state= paste0(state,region),\r\n         local = paste0(state, local))\r\n\r\n\r\n\r\nA sunburst chart is divided into concentric layers that are defined by the idof each grouping and its corresponding parent.\r\nWe define the first layer, that is the kernel of the circles with the level typefrom the hierarchy. Because it is the kernel it has no parents we leave this attribute blank. We define also an attribute hoverto display the number of clusters in each grouping on hover.\r\n\r\n\r\nshow\r\n\r\nlayer1 <- data %>% \r\n    group_by(type) %>% \r\n    summarise(n=n()) %>% \r\n    mutate(\r\n      ids = paste0('settlement', type),\r\n      labels = paste0('settlement <br>', type),\r\n      parents = '') %>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n) %>% \r\n  mutate(\r\n    hover= paste('\\n sample size', n)\r\n  )\r\n\r\nplot_ly(layer1, \r\n        ids = ~ids, \r\n        labels = ~labels, \r\n        parents = ~parents, \r\n        type = 'sunburst', \r\n        hovertext=~hover, insidetextorientation='radial')\r\n\r\n\r\n\r\n\r\nThe next level corresponds the region and has for parent the settlement type ids previously defined:\r\n\r\n\r\nshow\r\n\r\nsunburst_data_2layers <- rbind(\r\n  # first layer\r\n  layer1,\r\n  # second layer\r\n  data %>% \r\n    group_by(type, region) %>% \r\n    summarise(n=n()) %>% \r\n    mutate(\r\n      ids = paste('settlement', type, '-', 'region', region),\r\n      labels = paste0('region ', region),\r\n      parents = paste0('settlement', type))%>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n)%>% \r\n  mutate(\r\n    hover= paste(ids, '\\n sample size', n)\r\n  )) \r\n\r\nplot_ly(sunburst_data_2layers, \r\n        ids = ~ids, \r\n        labels = ~labels,\r\n        parents = ~parents, \r\n        type = 'sunburst', \r\n        hovertext=~hover, insidetextorientation='radial')\r\n\r\n\r\n\r\n\r\nTo get the full picture we add the two levels, stateand local .\r\n\r\n\r\nshow\r\n\r\n# create data for sunburst plot\r\nsunburst_data_4layers <- rbind(\r\n  # first layer\r\n  layer1,\r\n  # second layer\r\n  data %>% \r\n    group_by(type, region) %>% \r\n    summarise(n=n()) %>% \r\n    mutate(\r\n      ids = paste('settlement', type, '-', 'region', region),\r\n      labels = paste0('region ', region),\r\n      parents = paste0('settlement', type))%>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n)%>% \r\n  mutate(\r\n    hover= paste(ids, '\\n sample size', n)\r\n  ),\r\n  # third layer\r\n  data %>% \r\n    group_by(type, region, state) %>% \r\n    summarise(n=n()) %>% \r\n    mutate(\r\n      ids = paste('settlement', type, '-', 'region', region, '-', 'state', state, '-', 'region', region),\r\n      labels = paste0('state ', state),\r\n      parents = paste('settlement', type, '-', 'region', region))%>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n)%>% \r\n  mutate(\r\n    hover= paste(ids, '\\n sample size', n)\r\n  ),\r\n  # fourth layer\r\n  data %>% \r\n    group_by(type, region, state, local) %>% \r\n    summarise(n=n()) %>% \r\n    mutate(\r\n      ids = paste('settlement', type, '-', 'region', region, '-', 'state', state,  '-', local),\r\n      labels = paste0('local ', local),\r\n      parents = paste('settlement', type, '-', 'region', region, '-', 'state', state, '-', 'region', region))%>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n) %>% \r\n  mutate(\r\n    hover= paste(ids, '\\n sample size', n)\r\n  )\r\n)\r\n\r\nplot_ly(sunburst_data_4layers, \r\n        ids = ~ids, \r\n        labels = ~labels, \r\n        parents = ~parents, \r\n        type = 'sunburst', \r\n        hovertext=~hover, insidetextorientation='radial')\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Full grouping structure of the survey\r\n\r\n\r\n\r\nPlotly enables interaction with the chart: click on the grouping to get the detailed picture of the hierarchy.\r\nVisualising missing combinations\r\nIt is important to explore what are the missing combinations in the data:.\r\nto understand the characteristics of our sample\r\nto index correctly the model\r\nFor that purpose we will make use of the wonderful function from the tidyversesuite: complete.\r\nTo demonstrate, we will use the two first layers that are typeand region, such that we want to spot the regions that do not contain all settlement types:\r\n\r\n\r\nshow\r\n\r\n# create missing combinations\r\ndata_complete <- data %>% \r\n  complete(region, nesting(type))\r\n\r\ndata_complete %>% filter(is.na(local)) %>% kbl() %>% kable_minimal()\r\n\r\n\r\n\r\nregion\r\n\r\n\r\ntype\r\n\r\n\r\nid\r\n\r\n\r\npopulation\r\n\r\n\r\npop_density\r\n\r\n\r\nstate\r\n\r\n\r\nlocal\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n4\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n10\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n10\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n10\r\n\r\n\r\n4\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nWe see that two regions (4 and 10) comprise only settlement type 5.\r\nWe can visualise it with a sunburst chart by adding a color attribute:\r\n\r\n\r\nshow\r\n\r\nmakeSunburst2layer <- function(data){\r\n  layers <- rbind(\r\n  # first layer\r\n  data %>% \r\n    group_by(type) %>% \r\n    summarise(n=sum(!is.na(population))) %>% \r\n    mutate(\r\n      ids = paste0('settlement', type),\r\n      labels = paste0('settlement <br>', type),\r\n      parents = '') %>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n),\r\n  # second layer\r\n  data %>% \r\n    group_by(type, region) %>% \r\n    summarise(n=sum(!is.na(population))) %>% \r\n    mutate(\r\n      ids = paste('settlement', type, '-', 'region', region),\r\n      labels = paste0('region ', region),\r\n      parents = paste0('settlement', type))%>% \r\n    ungroup() %>% \r\n    select(ids,labels, parents,n)) %>%\r\n    mutate(\r\n      hover= paste(ids, '\\n sample size', n),\r\n      color= ifelse(n==0, 'yellow','')\r\n    )\r\n  \r\n return(layers)\r\n}\r\n\r\nplot_ly() %>% \r\n   add_trace(data=makeSunburst2layer(data), \r\n             ids = ~ids, labels = ~labels, parents = ~parents, \r\n             type = 'sunburst', \r\n             hovertext=~hover, marker= list(colors=~color),  \r\n             insidetextorientation='radial',\r\n             domain = list(column = 0)) %>% \r\n     add_trace(data=makeSunburst2layer(data_complete), \r\n             ids = ~ids, labels = ~labels, parents = ~parents, \r\n             type = 'sunburst', \r\n             hovertext=~hover, marker= list(colors=~color),  \r\n             insidetextorientation='radial',\r\n             domain = list(column = 1))  %>%  \r\n  layout(\r\n      grid = list(columns =2, rows = 1),\r\n      margin = list(l = 0, r = 0, b = 0, t = 0))\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Full grouping structure of the survey with missing groups\r\n\r\n\r\n\r\n\r\n\r\n\r\nLeasure, Douglas R., Warren C. Jochem, Eric M. Weber, Vincent Seaman, and Andrew J. Tatem. 2020a. “National Population Mapping from Sparse Survey Data: A Hierarchical Bayesian Modeling Framework to Account for Uncertainty.” Proceedings of the National Academy of Sciences 117 (39): 24173–79. https://doi.org/10.1073/pnas.1913050117.\r\n\r\n\r\nLeasure, Douglas R, Warren C Jochem, Eric M Weber, Vincent Seaman, and Andrew J Tatem. 2020b. “National Population Mapping from Sparse Survey Data: A Hierarchical Bayesian Modeling Framework to Account for Uncertainty.” Proceedings of the National Academy of Sciences.\r\n\r\n\r\nWeber, Eric M., Vincent Y. Seaman, Robert N. Stewart, Tomas J. Bird, Andrew J. Tatem, Jacob J. McKee, Budhendra L. Bhaduri, Jessica J. Moehl, and Andrew E. Reith. 2018. “Census-Independent Population Mapping in Northern Nigeria.” Remote Sensing of Environment 204 (January): 786–98. https://doi.org/10.1016/j.rse.2017.09.024.\r\n\r\n\r\nThe example sunburst chart is from: https://www.syncfusion.com/products/wpf/control/images/sfsunburst/sunburst.png)’)↩︎\r\n",
    "preview": "posts/2021-07-31-unraveling-hierarchichal-data-structure-with-sunburst-charts/./sunburst_2layers.png",
    "last_modified": "2024-03-20T11:41:06+00:00",
    "input_file": {},
    "preview_width": 661,
    "preview_height": 409
  },
  {
    "path": "posts/2021-08-01-woprvision/",
    "title": "woprVision: a shiny app to visualise Bayesian predictions",
    "description": "I helped develop woprVision, a multilingual interacive application to visualise and retrieve Bayesian population estimates. We use woprVision at WorldPop as part of our data release process to support (1) mean estimates spatial aggregation for people with limited GIS-skills (2) full distribution aggregation for coomputing correct credible intervals.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "academia",
      "dashboard"
    ],
    "contents": "\r\nwoprVision is an R shiny application that allows you to browse an interactive map to get population estimates for specific locations and demographic groups. It is\r\nIt has been developed for three reasons:\r\nvisualization of newly released population datasets\r\neasy interaction with gridded population for people with limited GIS skills through (1) a drawing function, (2) a geojson upload function.\r\ncomputation of adequate credible intervals for spatial aggregates from the full posterior distribution\r\nwoprVision is available online at https://apps.worldpop.org/woprVision.\r\nIts source code is hosted here: https://github.com/wpgp/wopr\r\n\r\nIf run with local files, woprVision requires a SQLlite database with population prediction distribution for every grid cell and a tiled image of the gridded population.\r\nOnline, it runs on a shiny server and pings the WorldPop API.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-01-woprvision/woprvision.PNG",
    "last_modified": "2025-09-10T17:01:44+01:00",
    "input_file": {},
    "preview_width": 1375,
    "preview_height": 609
  },
  {
    "path": "posts/2021-06-02-high-resolution-covariates-and-top-down-population-disagregation/",
    "title": "How fine can a top-down population model be? [draft]",
    "description": "We will study the use of very high-resolution covariates in top-down population model and its impact on population gridded estimates. We will examine building-footprint derived covariates in Sierra Leone population model. Disclaimer: This post is still being drafted.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2021-06-02",
    "categories": [
      "academia"
    ],
    "contents": "\r\nDisclaimer: This post is still being drafted.\r\nBackground\r\nTo obtain high-resolution population estimates, WorldPop developed a top-down method that disagregates population totals into grid cells thanks to ancillary geospatial covariates.\r\nThe method consists in fitting a random-forest model (Breiman 2001) at the finest administrative level for which population count is available. The estimated model is then used to predict population density for every grid cell thanks to the geospatial covariates. Finally, the predicted gridded population density layer plays the role of a weighting layer to breakdown the population totals into the grid cells (Stevens et al. 2015).\r\nTop-down disagregation modelThe geospatial covariates commonly used by WorldPop initative for mapping population globally (WorldPop Research Group et al. 2018) are based1:\r\non resampled rasters\r\nResampled DMSP-OLS night-time lights 2000-2011\r\nResampled VIIRS night-time lights 2012-2016\r\nSRTM-based elevation per country 2000\r\nSRTM-based slope per country 2000\r\nThe input rasters have a resolution ranging from 900m to 90 m at the Equateur (Lloyd et al. 2019).\r\n\r\n\r\n\r\nFigure 1: Exemple of raster resampling: Slope in Sierra Leone\r\n\r\n\r\n\r\non Euclidean distance measurement\r\nDistance to European Space Agency Land cover categories\r\nDistance to International Union for Conservation of Nature strict nature reserve and wilderness area edges\r\nDistance to open-water coastline per country\r\nDistance to OpenStreetMap (OSM) major roads, major road intersections and major waterways\r\nRasters resulting from Euclidian distance measurement can have virtually any spatial resolution. They however might not be representative of localized informative variations for population modelling.\r\n\r\n\r\n\r\nFigure 2: Exemple of Euclidean measurement: Distance to waterways in Sierra Leone\r\n\r\n\r\n\r\nheir distribution\r\nThe purpose of this post will be to study the impact of the building-footprint derived covariates on the modelling and subsequently on the gridded estimates in Sierra Leone.\r\nData\r\nRecently, WorldPop released openly gridded building patterns across entire sub-Saharan Africa at 100m resolution (Dooley and Tatem 2020). These were derived from building footprints extracted from satellite imagery with a spatial accuracy of 6m (Ecopia.AI and Maxar Technologies 2019). In top-down modelling, this data has already been used to produce an alternative global set of gridded population estimates constraining the estimates to settled grid cells2 (Bondarenko et al. 2020).\r\nMethod\r\n\r\n\r\n\r\nBondarenko, Maksym, David Kerr, Alessandro Sorichetta, and Andrew Tatem. 2020. “Census/Projection-Disaggregated Gridded Population Datasets for 51 Countries Across Sub-Saharan Africa in 2020 Using Building Footprints.”\r\n\r\n\r\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\r\n\r\n\r\nDooley, Claire A, and Andrew J Tatem. 2020. Gridded Maps of Building Patterns Throughout Sub-Saharan Africa, Version 1.0. WorldPop Research Group, University of Southampton. https://doi.org/10.5258/SOTON/WP00666.\r\n\r\n\r\nEcopia.AI, and Maxar Technologies. 2019. Digitize Africa Data.\r\n\r\n\r\nLloyd, Christopher T, Heather Chamberlain, David Kerr, Greg Yetman, Linda Pistolesi, Forrest R Stevens, Andrea E Gaughan, et al. 2019. “Global Spatio-Temporally Harmonised Datasets for Producing High-Resolution Gridded Population Distribution Datasets.” Big Earth Data 3 (2): 108139. https://doi.org/10.1080/20964471.2019.1625151.\r\n\r\n\r\nStevens, Forrest R., Andrea E. Gaughan, Catherine Linard, and Andrew J. Tatem. 2015. “Disaggregating Census Data for Population Mapping Using Random Forests with Remotely-Sensed and Ancillary Data.” PLOS ONE 10 (2): e0107042. https://doi.org/10.1371/journal.pone.0107042.\r\n\r\n\r\nWorldPop Research Group, Department of Geography and Geosciences, University of Louisville, Departement de Geographie, Universite de Namur, and Center for International Earth Science Information Network (CIESIN), Columbia University. 2018. “Global High Resolution Population Denominators Project - Funded by the Bill and Melinda Gates Foundation (OPP1134076).” https://doi.org/10.5258/SOTON/WP00645.\r\n\r\n\r\nThey can be downloaded at: https://www.worldpop.org/project/categories?id=14↩︎\r\nSettled cells are cells containing at least one building footprint.↩︎\r\n",
    "preview": "posts/2021-06-02-high-resolution-covariates-and-top-down-population-disagregation/dat/sle_slope.PNG",
    "last_modified": "2025-09-10T16:58:19+01:00",
    "input_file": {},
    "preview_width": 403,
    "preview_height": 400
  },
  {
    "path": "posts/2021-07-02-top-down-r-tutorial/",
    "title": "Small area population estimates using random forest top-down disaggregation: An R tutorial",
    "description": "I have co-authored an R tutorial on how to develop top-down population models, which consists in disaggregating administrative estimates to a finer spatial resolution. We  first shared this tutorial with the Brazilian Stats Office (IGBE).",
    "author": [
      {
        "name": "Douglas R Leasure",
        "url": {}
      },
      {
        "name": "Edith Darin",
        "url": {}
      },
      {
        "name": "Andrew J tatem",
        "url": {}
      }
    ],
    "date": "2020-12-08",
    "categories": [
      "academia",
      "tutorial"
    ],
    "contents": "\r\nNote: This blog post is a simple copy/paste of “Leasure DR, Darin E,\r\nTatem AJ. 2020. Small area population estimates using random forest\r\ntop-down disaggregation: An R tutorial. WorldPop, University of\r\nSouthampton. doi:10.5258/SOTON/WP00697”, published first\r\nhere.\r\nIntroduction\r\nThe purpose of top-down disaggregation is to estimate population counts\r\nat a finer spatial resolution than the available population totals for\r\nadministrative units. WorldPop top-down\r\ndisaggregation\r\nimplements a dasymetric mapping approach that uses the random forest\r\nmachine learning algorithm to disaggregate projected census totals to\r\nestimate population counts for 100 m grid cells (Stevens et al. 2015; Sorichetta et al. 2015). Dasymetric mapping estimates population counts at a\r\nfiner resolution than the input population totals based on relationships\r\nwith high resolution geospatial covariates like building locations and\r\nroad networks.\r\nIn this tutorial, we will demonstrate how to implement this method in\r\nthe R statistical programming environment. We will adapt the method to\r\nestimate population counts for census enumeration areas (EAs) rather\r\nthan 100 m grid cells. To demonstrate the approach, we will\r\ndissaggregate population totals from municipalities in Brazil to\r\nestimate populations in finer-scale census EAs (Fig.\r\n1).\r\n\r\n\r\n\r\nFigure 1: Example of enumeration area boundaries in Brazil\r\n\r\n\r\n\r\nPre-requisites\r\nThe following software may be needed to complete this tutorial:\r\nR environment,\r\nRStudio (optional), and\r\nArcGIS or QGIS\r\n(optional to view results).\r\nThe following pre-requisite skills may be helpful:\r\nR programming language\r\ninstall packages, load and manipulate data\r\n\r\nMachine learning concepts\r\npredictor and response variables, model diagnostics\r\n\r\nGIS concepts\r\nraster and vector data, zonal statistics\r\n\r\nBackground\r\nThe modelling process consists of three steps (Fig. 2):\r\nEstimate the relationship between population density and geospatial\r\ncovariates at the municipality level,\r\nPredict population densities at the level of enumeration areas using\r\nthe covariates, and\r\nUse predicted densities as a weighting layer to disaggregate the\r\npopulation totals for municipalities.\r\nThe core product is the weighting layer that should reflect the spatial\r\ndistribution of the population inside the municipalities. The final goal\r\nis to predict the population count in each enumeration area correctly.\r\nThe extent of the study area is defined by the spatial coverage of the\r\nmunicipalities for which we have population totals. The top-down method\r\ncannot estimate populations for enumeration areas within municipalities\r\nwhere population totals are unknown.\r\n\r\n\r\n\r\nFigure 2: Modelling Process\r\n\r\n\r\n\r\nRandom forest is a machine learning algorithm that builds an ensemble of\r\nindividual regression trees to predict values of a response variable\r\n(e.g. population density) based on its relationships with geospatial\r\ncovariates (Breiman 2001; Stevens et al. 2015). Each regression tree identifies\r\nthreshold values of the covariates that can be used to split the\r\ntraining data into fairly homogeneous groups (i.e. with similar\r\npopulation densities).\r\nThe algorithm prevents overfitting by withholding a subset of the\r\ntraining data for each individual tree that can be used to calculate\r\nrates of “out-of-bag” prediction error (i.e. out-of-sample\r\ncross-validation). The algorithm also prevents overfitting by randomly\r\nselecting a small set of covariates to be considered for splitting the\r\ntraining data at each node of the regression tree, rather than assessing\r\nall covariates at each split. These features make random forest fairly\r\nrobust to multi-collinearity among covariates allowing it to use large\r\nsets of covariates without a priori variable selection. However, a\r\nlarge number of covariates might impact running time, especially at the\r\nprediction stage, and it may be useful to select a subset of covariates\r\nbased on the measures of covariate importance provided by the random\r\nforest algorithm (Stevens et al. 2015; Bondarenko et al. 2018). While random forest\r\npredictions are robust to multi-collinearity among covariates, the\r\nmeasures of covariate importance may not be (Genuer, Poggi, and Tuleau-Malot 2010).\r\nTo dive deeper on the subject of random forest, here are some online\r\nmaterials:\r\nA use-R 2009 conference\r\npresentation\r\ngiven by Adele Cutler who developed the random forest algorithm with\r\nLeo Breiman.\r\nA 2019 blog\r\npost\r\nby Tony Yiu with clear explanations of Random Forest main features\r\nnamely bagging and covariates randomness that supports the\r\ncreation of uncorrelated forest of decision trees.\r\nA 2001\r\narticle\r\nfrom Breiman explaining the underlying maths.\r\nThe implementation we will provide in this tutorial will follow the\r\nguidance set up by Bondarenko et al. (2018) to apply random\r\nforest for top-down disaggregation.\r\nR Environment\r\nThis model will be implemented in the R programming language (R Core Team 2020).\r\nOur first step is to setup the R environment so that it contains the R\r\npackages and data that we will need. The randomForest package\r\n(Liaw and Wiener 2002) implements the random forest algorithm in R. If you have not\r\nalready installed it, then you will need to install it.\r\n\r\n\r\ninstall.packages('randomForest')\r\n\r\n\r\nOnce installed, you must load the randomForest package into your R\r\nenvironment.\r\n\r\n\r\nlibrary(randomForest)\r\n\r\n\r\nWe will define our working directory to be the location where we have\r\nstored the input data for the tutorial (master_train.csv and\r\nmaster_predict.csv described below).\r\n\r\n\r\nsetwd('c:/myfolder')\r\n\r\n\r\nSource Data\r\n\r\n\r\n\r\nThis tutorial includes two input data sets, provided as csv spreadsheets\r\n(comma-separate-values text files):\r\nmaster_train.csv Training data for the model that contains\r\npopulation totals and covariate values for 5568\r\nmunicipalities in Brazil, and\r\nmaster_predict.csv Prediction data for the model that contains\r\ncovariate values for 444215 enumeration areas\r\nwhere population estimates are needed.\r\nWe will load the datasets into our R environment using the read.csv\r\nfunction and display their top five rows with the head function:\r\n\r\n\r\n# training data from municipalities\r\nmaster_train <- read.csv(\"master_train.csv\")\r\n# covariates from enumeration areas\r\nmaster_predict <- read.csv(\"master_predict.csv\")\r\n\r\n\r\n\r\n\r\nhead(master_train[,1:5]) # only showing first five columns\r\n\r\n          name_muni geo_code     pop       area\r\n1       Bras\\xedlia  5300108 3055149 5779998938\r\n2  Vila Prop\\xedcio  5222302    5882 2181582535\r\n3          Vila Boa  5222203    6312 1060171349\r\n4 Vicentin\\xf3polis  5222054    8873  737256139\r\n5     Vian\\xf3polis  5222005   13977  954283662\r\n6         Varj\\xe3o  5221908    3838  519194233\r\n  mean.bra_viirs_100m_2016\r\n1               8.51690292\r\n2               0.08895954\r\n3               0.07032987\r\n4               0.13859431\r\n5               0.17625734\r\n6               0.09681950\r\n\r\nhead(master_predict[,1:4]) # only showing first four columns\r\n\r\n  EA_id geo_code mean.bra_viirs_100m_2016 mean.bra_srtm_slope_100m\r\n1     1  1100015                21.700214                 1.954565\r\n2     2  1100015                12.255162                 2.374444\r\n3     3  1100015                18.794325                 1.983213\r\n4     4  1100015                11.781239                 1.631729\r\n5     5  1100015                 3.152316                 2.861502\r\n6     6  1100015                 7.395526                 1.814885\r\n\r\nNotice that only the municipality-level dataset master_train.csv\r\ncontains a column for population. The EA-level dataset does not contain\r\na column for population, and it is the goal of this tutorial to estimate\r\nthose EA-level populations based on the EA-level covariates in\r\nmaster_predict.csv.\r\nAlso, notice that the column names for the covariates are exactly the\r\nsame in the municipality-level dataset and the EA-level dataset. All of\r\nthe municipality-level covariates used to train the model must also be\r\navailable at the EA level for the model predictions.\r\nThe geo_code column contains a numeric identifier for each\r\nmunicipality and will be used in later steps to identify the\r\nmunicipality that each EA belongs to. Boundaries of enumeration areas in\r\nBrazil roughly follow municipality boundaries (i.e. enumeration areas\r\nare nested within municipalities), but the boundaries are not harmonized\r\nexactly. We assigned each enumeration area to the municipality that\r\ncontained the majority of its area.\r\nNote: We derived the two spreadsheets (master_train.csv and\r\nmaster_predict.csv) from the following sources:\r\nBrazil’s municipality\r\nboundaries\r\n(IBGE 2019),\r\nBrazil’s 2020 census\r\nprojections\r\nfor municipalities (IBGE 2020b),\r\nBrazil’s census EA\r\nboundaries\r\n(IBGE 2020a),\r\nWorldPop’s\r\nmastergrid\r\nfor Brazil (WorldPop and CIESIN 2018a), and\r\nWorldPop’s geospatial covariate\r\nrasters\r\nfor Brazil (Lloyd et al. 2019; Lloyd, Sorichetta, and Tatem 2017).\r\nSee the section Zonal Statistics for more information about how the\r\ninput data were created.\r\nRandom Forest\r\nWe will reformat the source data into the correct format for the random\r\nforest model before we fit the model and apply it to estimate\r\npopulations in each enumeration area (EA). We will start by preparing\r\nthe response and predictor variables from master_train.csv for\r\ntraining the model.\r\nResponse Variable\r\nThe response variable y_data for our random forest model must be a\r\nvector with a population value for each municipality. We will define our\r\nresponse variable as the log of population density:\r\n\r\n\r\ny_data <- master_train$pop / master_train$area\r\n\r\ny_data <- log(y_data)\r\n\r\n\r\nWe use log population density as the response variable rather than\r\npopulation counts for two main reasons. First, population densities are\r\nmore comparable than counts among spatial units of varying sizes (e.g.\r\nmunicipalities and EAs). Second, the logarithm transformation reshapes\r\nthe response variable as a Gaussian distribution, better inline with the\r\ndistributions of covariates (Stevens et al. 2015) (Fig. 3). Note\r\nthat master_train$area is measured in square meters in this case but\r\nany unit of area will work.\r\n\r\n\r\n\r\nFigure 3: Histogram of the log-transformed population density.\r\n\r\n\r\n\r\nWe now have y_data, a vector with the log population density for every\r\nmunicipality in Brazil.\r\nPredictor Variables\r\nThe predictor data x_data for our random forest model must be a\r\ndata.frame with a row for each municipality and a column for each\r\ncovariate. The row order must match that of y_data (i.e. row 1 from\r\nx_data must represent the same enumeration area as element 1 from\r\ny_data).\r\nWe build the predictor data x_data by subsetting the covariates from\r\nmaster_train.csv that we would like to include in our model. Remember,\r\nwe want to select covariates that will be good predictors of log\r\npopulation density.\r\n\r\n\r\n# list all covariate names (i.e. column names)\r\ncols <- colnames(master_train)\r\nprint(cols)\r\n\r\n [1] \"name_muni\"                         \r\n [2] \"geo_code\"                          \r\n [3] \"pop\"                               \r\n [4] \"area\"                              \r\n [5] \"mean.bra_viirs_100m_2016\"          \r\n [6] \"mean.bra_srtm_slope_100m\"          \r\n [7] \"mean.bra_osm_dst_road_100m_2016\"   \r\n [8] \"mean.bra_esaccilc_dst200_100m_2015\"\r\n [9] \"mean.bra_esaccilc_dst190_100m_2015\"\r\n[10] \"mean.bra_esaccilc_dst150_100m_2015\"\r\n[11] \"mean.bra_esaccilc_dst140_100m_2015\"\r\n[12] \"mean.bra_esaccilc_dst130_100m_2015\"\r\n[13] \"mean.bra_esaccilc_dst040_100m_2015\"\r\n[14] \"mean.bra_esaccilc_dst011_100m_2015\"\r\n[15] \"code_state\"                        \r\n[16] \"code_muni\"                         \r\n[17] \"abbrev_state\"                      \r\n\r\nOur example data master_train.csv includes a few columns that are not\r\ncovariates (e.g. geo_code) and so we do not want to include them in\r\nx_data. Our covariates all include the word “mean” in their column\r\nnames because they were calculated as the mean value of underlying\r\ncovariate rasters within each municipality. So, we will use “mean” as a\r\nsearch term to identify the correct columns. This provides an example of\r\nselecting a subset of the predictors, but you could use other subsets,\r\nsearch terms, or methods of subsetting.\r\n\r\n\r\n# select column names that contain the word 'mean'\r\ncov_names <- cols[grepl('mean', cols)] \r\nprint(cov_names)\r\n\r\n [1] \"mean.bra_viirs_100m_2016\"          \r\n [2] \"mean.bra_srtm_slope_100m\"          \r\n [3] \"mean.bra_osm_dst_road_100m_2016\"   \r\n [4] \"mean.bra_esaccilc_dst200_100m_2015\"\r\n [5] \"mean.bra_esaccilc_dst190_100m_2015\"\r\n [6] \"mean.bra_esaccilc_dst150_100m_2015\"\r\n [7] \"mean.bra_esaccilc_dst140_100m_2015\"\r\n [8] \"mean.bra_esaccilc_dst130_100m_2015\"\r\n [9] \"mean.bra_esaccilc_dst040_100m_2015\"\r\n[10] \"mean.bra_esaccilc_dst011_100m_2015\"\r\n\r\n# subset the data.frame to only these columns\r\nx_data <- master_train[,cov_names]\r\nhead(x_data[,1:2]) # only showing first two columns\r\n\r\n  mean.bra_viirs_100m_2016 mean.bra_srtm_slope_100m\r\n1               8.51690292                 3.547058\r\n2               0.08895954                 4.198463\r\n3               0.07032987                 3.522752\r\n4               0.13859431                 1.620756\r\n5               0.17625734                 3.577609\r\n6               0.09681950                 5.659529\r\n\r\nWe now have a data.frame x_data with a row for each municipality and a\r\ncolumn for each covariate.\r\nModel Fitting\r\nThe model fitting process will identify relationships between the\r\nresponse variable (i.e. population) and predictor variables at the\r\nmunicipality level.\r\nSettings\r\nBefore running the random forest model, we need to identify appropriate\r\nvalues for each argument of the randomForest() and/or tuneRF()\r\nfunctions. See ?randomForest and ?tuneRF for detailed descriptions\r\nof all arguments to the functions.\r\nA critical argument in randomForest() function is mtry, the number\r\nof randomly selected variables for evaluating the best split at each\r\nnode in a regression tree within the random forest model. This process\r\nis at the root of the randomness in random forest models. To find the\r\noptimal mtry value, we use the function tuneRF() that compares\r\ndifferent models based on their “out-of-bag” error rates.\r\nNote: Out-of-bag error is a form of cross-validation that is\r\ncalculated by comparing model predictions to observed data from EAs that\r\nwere randomly selected to be witheld from model fitting (i.e. held\r\n“out-of-bag”) for an iteration of the model (i.e. a single regression\r\ntree in the random forest model).\r\nDuring each iteration of a random forest model, a regression tree is fit\r\nto a random sub-sample of the data. This prevents overfitting and also\r\nprovides a built-in mechanism for cross-validation. The sampsize\r\nargument sets the sample size, and the replace argument defines the\r\nsampling strategy. In our case, we will define sampsize as the total\r\ncount of observations (i.e. 5568 municipalities) and\r\nreplace will be TRUE to sample with replacement from the training\r\ndata.\r\nOur specifications for other parameters follow Bondarenko et al.\r\n(2018).\r\nntree: Number of trees to grow. There is no issue of overfitting\r\nwhen adding additional trees. We opt for 500, the default value of\r\nthe randomForest() function.\r\nimportance: If TRUE, the model will calculate the covariate\r\nimportance for further analysis. We opt for TRUE.\r\nnodesize: Minimum size of the terminal node of the trees\r\ncontrols the tree complexity. We make it about 0.1% of the training\r\ndata sample size (i.e. 5).\r\nRun the Model\r\nOnce we have decided on appropriate settings, we will input them as\r\narguments to the tuneRF() function (note: this may take several\r\nminutes to run).\r\n\r\n\r\n# model fitting\r\npopfit <- tuneRF(\r\n  x=x_data, \r\n  y=y_data, \r\n  plot=TRUE, \r\n  mtryStart=length(x_data)/3, \r\n  ntreeTry=500, \r\n  improve=0.0001, # threshold on the OOB error to continue the search\r\n  stepFactor=1.20, # incremental improvement of mtry\r\n  trace=TRUE, \r\n  doBest=TRUE, # last model trained with the best mtry\r\n  nodesize=length(y_data)/1000, \r\n  na.action=na.omit, \r\n  importance=TRUE, # calculate variable importance\r\n  sampsize=length(y_data), # size of the sample to draw for OOB\r\n  replace=TRUE\r\n                 ) # sample with replacement\r\n\r\n\r\n\r\n\r\n\r\nThe popfit object contains the fitted random forest model. We can see\r\nthe items within the object:\r\n\r\n\r\nnames(popfit)\r\n\r\n [1] \"call\"            \"type\"            \"predicted\"      \r\n [4] \"mse\"             \"rsq\"             \"oob.times\"      \r\n [7] \"importance\"      \"importanceSD\"    \"localImportance\"\r\n[10] \"proximity\"       \"ntree\"           \"mtry\"           \r\n[13] \"forest\"          \"coefs\"           \"y\"              \r\n[16] \"test\"            \"inbag\"          \r\n\r\nWe can retrieve individual values, such as the value of the mtry\r\nsetting that was selected by the tuneRF function.\r\n\r\n\r\npopfit$mtry\r\n\r\n[1] 4\r\n\r\nSee the “Value” section in the help for the randomForest function\r\n?randomForest::randomForest for an explanation of each value.\r\nWe will now save the popfit object to our hard drive.\r\n\r\n\r\nsave(popfit, file='popfit.Rdata')\r\n\r\n\r\nWe can load the fitted model back into our R environment later using:\r\n\r\n\r\nload('popfit.Rdata')\r\n\r\n\r\nWeighting Layer\r\nWe will use the fitted model to create a weighting layer that will be\r\nused to redistribute the population totals for municipalities into the\r\nenumeration areas (EA) within them. The EA-level covariate data\r\nmaster_predict.csv contains 444215 rows, one for\r\neach EA in Brazil.\r\nWe will first use the fitted random forest model to generate raw model\r\npredictions for each EA and save them as a column in the\r\nmaster_predict data.frame:\r\n\r\n\r\n# random forest predictions\r\nmaster_predict$predicted <- predict(\r\n  popfit, \r\n  newdata = master_predict\r\n  )\r\n\r\n\r\nThe weights are then calculated from the model predictions using the\r\nfollowing formula:\r\n\\[\\begin{equation}\r\nweight_{i} = \\frac{exp(predicted_{i})}{\\sum_{i=1}^{I_j}{exp(predicted_{i})}}\r\n\\tag{1}\r\n\\end{equation}\\]\r\nwhere \\(predicted_i\\) is the model prediction for enumeration area \\(i\\),\r\nand \\(I_j\\) is the total number of enumeration areas in municipality \\(j\\).\r\nThese weights represent the proportion of the population from\r\nmunicipality \\(j\\) that lives in enumeration area \\(i\\).\r\nRedistribution to EA-level\r\nWe will now use the weights to estimate the population for each\r\nenumeration area \\(i\\) using the following formula:\r\n\\[\\begin{equation}\r\npopulation_{i} = total_j \\times weight_{i}\r\n\\tag{2}\r\n\\end{equation}\\]\r\nwhere \\(total_j\\) is the population total for municipality \\(j\\) and\r\n\\(weight_i\\) is the proportion of the population that lives in enumeration\r\narea \\(i\\) (Eq. (1)). We will now work through this formula\r\nstep-by-step in R.\r\nFirst, we will exponentiate the predictions (numerator from Eq.\r\n(1)) and save them as a column in master_predict:\r\n\r\n\r\n# back-transform predictions to natural scale\r\nmaster_predict$predicted_exp <- exp(\r\n  master_predict$predicted\r\n  )\r\n\r\n\r\nWe need to exponentiate the predictions because we log-transformed the\r\npopulation densities that we used as the response variable for training\r\nthe model. Therefore, the model predictions are population densities on\r\nthe log-scale. We exponentiate them to derive the predicted population\r\ndensities that we will use as weighting factors to disaggregate the\r\nmunicipality-level population totals.\r\nNext, we will sum the predicted population densities among EAs in each\r\nmunicipality (denominator from Eq. (1)) and merge the\r\nresults into master_predict using the geo_code column to match\r\nmunicipalities:\r\n\r\n\r\n# sum exponentiated predictions among EAs in each municipality\r\npredicted_exp_sum <- aggregate(list(predicted_exp_sum=master_predict$predicted_exp), \r\n                               by = list(geo_code=master_predict$geo_code), \r\n                               FUN = sum)\r\n\r\n# merge predicted_exp_sum into master_train based on geo_code\r\nmaster_predict <- merge(master_predict, \r\n                        predicted_exp_sum, \r\n                        by='geo_code')\r\n\r\n\r\nThese sums (i.e. the denominator from Eq. (1)) are used to\r\nscale the exponentiated model predictions to sum to one among EAs in\r\neach municipality.\r\nThen, we will merge the total populations for each municipality into\r\nmaster_predict, again using the geo_code column to match EAs to the\r\ncorrect municipalities:\r\n\r\n\r\n# merge municipality total populations from master_train into master_predict\r\nmaster_predict <- merge(master_predict,\r\n                        master_train[,c('geo_code','pop')],\r\n                        by = 'geo_code')\r\n\r\n# modify column name\r\nnames(master_predict)[ncol(master_predict)] <- 'pop_municipality'\r\n\r\n\r\nWe have now added all of the required information from Eqs.\r\n(2) and (1) into the master_predict\r\ndata.frame. We will calculate the EA-level population estimates and add\r\nthem to master_predict as a column:\r\n\r\n\r\n# calculate EA-level population estimates\r\nmaster_predict$predicted_pop <- with(master_predict, predicted_exp / predicted_exp_sum * pop_municipality)\r\n\r\n\r\nNotice that this line of code is equivalent to Eqs. (2)\r\nand (1).\r\nWe have now disaggregated the population totals from\r\n5568 municipalities in Brazil into population\r\nestimates for 444215 enumeration areas (Fig.\r\n4).\r\n\r\n\r\n\r\nFigure 4: Census-based population totals for municipalities (left) that have been disaggregated into population estimates for enumeration areas (right) east of Sao Paulo, Brazil. Darker red corresponds to more people.\r\n\r\n\r\n\r\nDiagnostics\r\nSumming Enumeration Areas\r\nWe first want to check that the EA-level population estimates sum to the\r\nmunicipality-level population totals. To do this, we will aggregate the\r\nEA-level predictions using the geo_code column to identify the\r\nmunicipalities that each EA belongs to:\r\n\r\n\r\n# sum EA population estimates within each municipality\r\ntest <- aggregate(master_predict$predicted_pop, \r\n                  by = list(geo_code=master_predict$geo_code), \r\n                  FUN = sum)\r\n\r\n# modify column names\r\nnames(test) <- c('geo_code','predicted_pop')\r\n\r\n# merge municipality population totals\r\ntest <- merge(test,\r\n              master_train[,c('geo_code','pop')],\r\n              by = 'geo_code')\r\n\r\n# test if estimates match muncipality population totals\r\nall(test$pop == round(test$predicted_pop))\r\n\r\n[1] TRUE\r\n\r\nIf we fail this test (i.e. if the last line of code returns a value of\r\nFALSE), it could mean that there was an error in our implementation of\r\nEq. (2) or Eq. (1) because those\r\nequations should re-scale the EA-level population estimates so that they\r\nalways sum to the population totals for each municipality. This test\r\ndoes not assess the goodness-of-fit for the random forest model.\r\nGoodness-of-Fit\r\nThe print function from the randomForest package displays two metrics\r\nthat describe the out-of-bag prediction residuals:\r\nthe mean squared residuals (MSE)\r\nthe % of variance explained which corresponds to a pseudo \\(R^2\\),\r\n(1 - MSE/ Var(y_data)).\r\nThe first metric (MSE) is dependent on the scale of the input response\r\nvariable (e.g. the unit of area used to calculate population density),\r\nwhereas the second metric (\\(R^2\\)) can be compared across models with\r\ndifferent response variables.\r\n\r\n\r\n# goodness-of-fit metrics\r\nprint(popfit) \r\n\r\n\r\nCall:\r\n randomForest(x = x, y = y, mtry = res[which.min(res[, 2]), 1],      replace = TRUE, sampsize = ..4, nodesize = ..1, importance = TRUE,      na.action = ..2) \r\n               Type of random forest: regression\r\n                     Number of trees: 500\r\nNo. of variables tried at each split: 4\r\n\r\n          Mean of squared residuals: 0.2089229\r\n                    % Var explained: 89.97\r\n\r\nWe can visualize the out-of-bag predictions for municipalities.\r\nFirst we plot the observed vs predicted values. This enables us to spot\r\nany municipalities where the random forest predictions are not accurate.\r\n\r\n\r\n# plot observed vs predicted (out-of-bag)\r\nplot(x = y_data,\r\n     y = predict(popfit), \r\n     main = 'Observed vs Predicted log-Densities')\r\n\r\n# 1:1 line\r\nabline(a=0, b=1, col='red')\r\n\r\n\r\n\r\nNote: The predict function in this code will return “out-of-bag”\r\npredictions (i.e. weights) for every municipality because we did not\r\ngive it a new prediction data set, like we did in the section Weighting\r\nLayer (see?predict.randomForest for more info). The “out-of-bag”\r\nprediction for a municipality is generated from the regression trees of\r\nthe model that did not include that municipality in their randomly\r\nsampled training data. This provides a cross-validation (a.k.a.\r\nout-of-bag, or out-of-sample) assessment.\r\nPlotting the residuals versus the predictions allows us to detect any\r\npattern towards over or underestimation.\r\n\r\n\r\n# plot residuals (out-of-bag)\r\nplot(x = predict(popfit), \r\n     y = y_data - predict(popfit), \r\n     main = 'Residuals vs Predicted',\r\n     ylab = 'Out-of-bag residuals',\r\n     xlab = 'Out-of-bag prediction')\r\n\r\n# horizontal line at zero\r\nabline(h=0, col='red')\r\n\r\n\r\n\r\nImportant Note: The different goodness-of-fit diagnostics presented\r\nhere are focused on the training dataset and thus on predicting at the\r\nmunicipality level, but remember that the final goal is to predict at\r\nthe enumeration area level.\r\nEA-level Assessment\r\nThere are two ways to evaluate the disaggregation results at the\r\nEA-level:\r\nComparing to an independent sample of enumeration areas or any other\r\nfine-scale spatial units for which we know the population counts, or\r\nBuild a test model using aggregated population counts at a coarser\r\nresolution (e.g. state-level) and evaluating its predictions at the\r\nmunicipality level.\r\nThe first option relies on the availability of an additional dataset.\r\nDisaggregating population estimates into 100 m grids rather than\r\nenumeration area polygons (as in this tutorial) would provide greater\r\nflexibility for aggregating units to match the spatial units of the\r\nindependent validation data.\r\nThe second option is discussed by Stevens et al. (2015). Its\r\neffectiveness depends on the size of the aggregated training dataset\r\n(e.g. the number of states). Indeed a coarser scale means a decreasing\r\nnumber of input observations, which undermines the quantity of\r\ninformation the random forest can extract.\r\nCovariate Importance\r\nWe can look at the influence of individual covariates on model\r\npredictions. There are two measures of variable importance available.\r\n“Type 1” covariate importance is computed as the difference between\r\nout-of-bag mean squared error (MSE) from the full model compared to a\r\nmodel in which the variable is randomly permuted for each tree. If MSE\r\nis not affected by random permutations of the variable, then it has low\r\nimportance according to this metric.\r\n\r\n\r\n# for covariate importance\r\nvarImpPlot(popfit, type=1) \r\n\r\n\r\n\r\n“Type 2” covariate importance is computed as the total decrease in node\r\npurity when the variable is used for a split in the regression trees.\r\nNode purity is defined as the residual sum of squares among samples that\r\nare classified into the same terminal node of a regression tree. If\r\nsplits in the tree based on this covariate do not decrease the residual\r\nsum of squares, then it has low importance according to this metric.\r\n\r\n\r\n# for covariate importance\r\nvarImpPlot(popfit, type=2) \r\n\r\n\r\n\r\nSee ?randomForest::importance for more details.\r\nNote: Use caution when interpreting covariate importance. A high\r\nimportance value indicates that the covariate improves predictive\r\nperformance, but does not indicate a causal relationship with spatial\r\npatterns of population density. Covariate importance scores may also be\r\nunderestimated for large groups of correlated predictor variables.\r\nLimitations\r\nRandom Forest can be time-intensive for large datasets.\r\nIf running time becomes an issue, there are several solutions that\r\nare outlined in the random forest set-up of Bondarenko et al.\r\n(2018).\r\nIf the model takes too long to be fitted, one can play with the\r\nparameter sampsize, which has a great impact on modelling time or\r\nntreeSize. Be careful though as a small number of trees can lead\r\nto overfitting. This issue can be clearly visible in any out-of-bag\r\ngoodness-of-fit metrics.\r\nIf the prediction takes too much time, Stevens et al.\r\n(2015) proposes to prune the number of covariates based on\r\ntheir importance metric. We have also provided an example of how to\r\ngenerate predictions more efficiently using parallel processing (see\r\nParallel Processing section below).\r\nRandom Forest models are not good at extrapolating.\r\nThey can only predict values that are in the range of observed\r\nvalues. This issue becomes a problem when the training and\r\nprediction inputs differ in range and distribution, which is known\r\nas covariate shift. The following plot can help detect a mismatch\r\nin covariates distribution between the two scales.\r\n\r\n\r\n# two panel layout\r\nlayout(matrix(1:2, nrow=1))\r\n\r\n# loop through two covariates\r\nfor(cov_name in c('mean.bra_srtm_slope_100m', 'mean.bra_viirs_100m_2016')){\r\n  \r\n  # combine EA-level and municipality-level values into a single vector\r\n  y <- c(master_predict[,cov_name], master_train[,cov_name])\r\n  \r\n  # create corresponding vector identifying spatial scale\r\n  x <- c(rep('EA', nrow(master_predict)),\r\n     rep('Municipality', nrow(master_train)))\r\n  \r\n  # create boxplot\r\n  boxplot(y~x, xlab=NA, ylab=cov_name)\r\n}\r\n\r\n\r\n\r\nFigure 5: The distributions of two covariates at each spatial scale: municipalities and enumeration areas. The left panel shows similar distributions across scales, but the right panel shows a covariate with significantly different distributions when measured for municipalities compared to enumeration areas.\r\n\r\n\r\n\r\nNotice that the covariate mean.bra_viirs_100m_2016 has a different\r\nrange of values for EAs compared to municipalities (Fig.\r\n5). This covariate measures the intensity of\r\nnighttime lights and we would expect its average values to be less\r\nin larger municipalities with large unsettled sections compared to\r\nsmall EAs, many of which are densely populated. This covariate was\r\nalso the most influential on model predictions (see Covariate\r\nImportance). We may want to consider re-calculating this covariate\r\nusing a mask to exclude unsettled areas within municipalities and\r\nEAs in an attempt to have a more comparable measure between spatial\r\nscales.\r\nRandom Forest models are not ideal for inferring covariate\r\neffects.\r\nThe purpose of the modelling is to have a great predictive\r\nperformance. We are not aiming at explaining the drivers of human\r\nspatial distribution. Therefore no causal effect should be derived\r\nfrom the modelling results.\r\nTips and Tricks\r\nThe previous sections demonstrated the basic components of random forest\r\ntop-down disaggregation, but there are additional tips and tricks that\r\nmay be useful.\r\nThis section utilizes several source files that we did not include with\r\nthe tutorial because they are large files that are publicly available\r\nfrom their original sources. These source files are:\r\nEnumeration area\r\nboundaries\r\nfor Brazil (IBGE 2020a),\r\nVIIRS nighttime\r\nlights\r\nfrom 2016 (WorldPop and CIESIN 2018b), and\r\nWorldPop\r\nmastergrid\r\nfor Brazil (WorldPop and CIESIN 2018a).\r\nNote: The code below will assume that these source files have been\r\ndownloaded and unzipped (if applicable) into your working directory.\r\nMap Results\r\nIf you want to view the EA-level population estimates on a map (e.g.\r\nFig. 4), you will need to join the model results (from\r\nsection Redistribution to EA-level above) to polygon features for use\r\nwith GIS software (e.g. Esri polygon shapefile).\r\nFor this step, we will need to install and load the sf package\r\n(Pebesma 2018) to read and manipulate GIS vector data (census EAs,\r\nin this case).\r\n\r\n\r\ninstall.packages('sf')\r\nlibrary('sf')\r\n\r\n\r\nThen, we need to load a vector GIS data set containing the polygons that\r\nwe would like to join our model results to. For this example, we are\r\nusing Brazilian census enumeration area polygons which are openly\r\navailable online as a polygon shapefile (IBGE 2020a).\r\n\r\n\r\nsf_polygons <- st_read('BR_Setores_2019.shp')\r\n\r\n\r\nNote: The st_read function can read most GIS file formats (e.g.\r\nEsri shapefile, geopackage, geojson). See ?sf::st_read for more\r\ninformation.\r\nWe will add an ID column that matches the enumeration area IDs from the\r\nEA_id column of the master_predict data.frame object (continuing\r\nfrom previous sections of this tutorial).\r\n\r\n\r\nsf_polygons$EA_id <- 1:nrow(sf_polygons)\r\n\r\n\r\nThen, we can merge the master_predict data.frame which contains our\r\nEA-level population estimates into sf_polygons. We will use the\r\nEA_id column to match values to the correct enumeration area polygons.\r\n\r\n\r\nsf_polygons <- merge(sf_polygons, \r\n                     master_predict,\r\n                     by='EA_id')\r\n\r\n\r\nFinally, we save the results to disk as an Esri shapefile.\r\n\r\n\r\nst_write(sf_polygons,\r\n         'master_predict_polygons.shp')\r\n\r\n\r\nThis will save the result as an Esri shapefile because it uses the\r\n.shp file extension. If we want to save to a different format, we can\r\nsimply modify the file extension. For example, we can save the result as\r\na GeoPackage by adding the.gpkg file extension.\r\n\r\n\r\nst_write(sf_polygons,\r\n         'master_predict_polygons.gpkg')\r\n\r\n\r\nZonal Statistics\r\nThis tutorial began with two pre-processed spreadsheets\r\nmaster_train.csv and master_predict.csv. These input data were\r\nderived from raster-based covariates using zonal statistics to summarize\r\nraster values within each municipality and enumeration area using mean\r\nvalues or other summary statistics (e.g. median, mode, standard\r\ndeviation, min, max). We will give a brief demonstration of how to do\r\nthis.\r\nWe will rely on the raster and exactextractr packages\r\n(Hijmans 2020; Daniel Baston 2020) for this step and so we\r\nwill first install and load those packages:\r\n\r\n\r\ninstall.packages(c('raster','exactextractr'))\r\nlibrary('raster')\r\nlibrary('exactextractr')\r\n\r\n\r\nWe already have polygon features sf_polygons loaded from the previous\r\nsection (Map Results). We will also need to load a covariate raster.\r\nWe will use a nighttime lights raster for Brazil that is openly\r\navailable (WorldPop and CIESIN 2018b).\r\n\r\n\r\nraster_covariate <- raster('bra_viirs_100m_2016.tif')\r\n\r\n\r\nNext, we will calculate a zonal mean within each enumeration area using\r\nthe object sf_polygons (from section Map Results).\r\n\r\n\r\nsf_polygons$mean.bra_viirs_100m_2016 <- exact_extract(x = raster_covariate,\r\n                                                      y = sf_polygons,\r\n                                                      fun = 'mean')\r\n\r\n\r\nHere, we saved the results into a new column of the sf_polygons\r\ndata.frame called mean.bra_viirs_100m_2016.\r\nNote: You may receive a warning message:\r\nPolygons transformed to raster CRS (EPSG:NA). This indicates that the\r\nprojection for sf_polygons was different than raster_covariate, and\r\nthat a reprojection was performed internally by exact_extract. This\r\ndoes not affect our results, but we could have avoided the warning\r\nmessage by first reprojecting the covariate raster to match the polygons\r\n(see ?raster::projectRaster) or by reprojecting the polygons to match\r\nthe covariate raster (see ?sf::st_transform).\r\nAfter we have calculated all of the zonal statistics that we need, we\r\ncan save the sf_polygons data.frame to disk as a csv spreadsheet.\r\n\r\n\r\nwrite.csv(st_drop_geometry(sf_polygons), \r\n          file = 'EA_covariates.csv',\r\n          row.names = FALSE)\r\n\r\n\r\nThis would be comparable to master_predict.csv that we used as input\r\ndata for this tutorial. Note that the st_drop_geometry() function\r\nremoves the polygon geometries from the sf object so that it is a\r\nsimple data.frame with no spatial geometries (i.e. polygons) associated\r\nwith each row. We could also save the results as an Esri shapefile to\r\nmaintain the feature geometries.\r\n\r\n\r\nst_write(sf_polygons, 'EA_covariates.shp')\r\n\r\n\r\nNote: The exactextractr\r\nREADME\r\nalso demonstrates a simple workflow to calculate zonal statistics for\r\nBrazilian municipalities.\r\nGridded Population Estimates\r\nIn this tutorial, we disaggregated municipality-level population totals\r\ninto finer resolution EA-level population estimates. We can also\r\ndisaggregate population totals into grid cells (e.g. 100 m, 1 km). It is\r\noften easier to work with gridded population estimates because they can\r\nbe aggregated to any areal units (e.g. census enumeration areas,\r\nadministrative units, health facility catchment areas) and they may also\r\nbe easier to compare to gridded data from other sources.\r\nTo do this, we need to create a new version of master_predict.csv\r\nwhere each row represents a grid cell rather than an enumeration area.\r\nWe will start by loading a raster to use as a mastergrid that defines\r\nthe locations and sizes of grid cells that we want to use.\r\n\r\n\r\nmastergrid <- raster('bra_level0_100m_2000_2020.tif')\r\n\r\n\r\nThis mastergrid raster for Brazil is publicly available from WorldPop\r\n(2018a). In this mastergrid, any cells that contain a\r\nvalue of NA are outside of Brazil.\r\nCell IDs for a raster are a numerical sequence starting from 1 in the\r\ntop-left corner of the raster and increasing left-to-right. There are\r\nmore than 2.5 billion grid cells in Brazil’s mastergrid and more than\r\none billion contain non-NA values. We want to identify the cell IDs for\r\nall cells with a non-NA value. Because Brazil contains so many grid\r\ncells, we will subset the mastergrid for processing.\r\n\r\n\r\ncells <- which(!is.na(mastergrid[1:1e7]))\r\n\r\n\r\nThe new vector object cells will contain the cell IDs from the\r\nmastergrid that contain non-NA values. We are only processing the first\r\n10 million grid cells (i.e. cells 1 to 1e7) in the mastergrid. We will\r\ncontinue the example using only this subset of the data to demonstrate\r\nhow to generate gridded population estimates. In practice, we may want\r\nto process all of the grid cells at once (if computing power allows) or\r\nloop through one subset of cell IDs at a time (e.g. using a for loop)\r\nuntil all cell IDs are processed.\r\nWe will now setup a data.frame with a column for the cell IDs.\r\n\r\n\r\nmastergrid_predict <- data.frame(row.names = cells)\r\n\r\n\r\nNext, we will use the same covariate raster raster_covariate from the\r\nprevious section (Zonal Statistics) (WorldPop and CIESIN 2018b). This\r\nraster has the exact same extent and cell size as the mastergrid.\r\nBecause the two rasters have identical extent and cell size, their cell\r\nIDs are comparable allowing us to use our vector of cell IDs cells to\r\nextract values very quickly and save them as a new column in the\r\ndata.frame master_predict.\r\n\r\n\r\nmastergrid_predict[cells, 'bra_viirs_100m_2016'] <- raster_covariate[cells]\r\n\r\n\r\nIf the extent and cell size of a covariate raster do not match, then we\r\ncan extract values from the covariate raster using the point coordinates\r\nassociated with each grid cell rather than using cell IDs, but this is a\r\nslower process.\r\n\r\n\r\n# XY coordinates of each grid cell\r\nxy <- xyFromCell(mastergrid, cells)\r\n\r\n# extract raster values at those coordinates  \r\nmastergrid_predict[cells, 'bra_viirs_100m_2016_alt'] <- extract(raster_covariate, xy)\r\n\r\n\r\nNote: Another option would be to resample the covariate raster to\r\nmatch the mastergrid, but exploring the different options for resampling\r\nis beyond the scope of this tutorial (see ?raster::resample for more\r\ninformation).\r\nWe can then save the results as a csv spreadsheet.\r\n\r\n\r\nwrite.csv(mastergrid_predict,\r\n          file = 'mastergrid_predict.csv',\r\n          row.names = FALSE)\r\n\r\n\r\nThis spreadsheet could be used in exactly the same way as\r\nmaster_predict.csv was used above to generate model predictions (see\r\nsection Weighting Layer). Because there are so many grid cells in\r\nBrazil where predictions are needed we would likely want to use parallel\r\nprocessing to speed up the calculations (see Parallel Processing\r\nsection below). With either approach, we would generate a new column\r\ncalled predicted_pop in the master_predict data.frame that would\r\ncontain the population estimates for each grid cell.\r\nAfter we have these model predictions in tabular form in\r\nmaster_predict, we would then want to rasterize those results. First,\r\nwe create an empty raster.\r\n\r\n\r\nraster_predict <- raster(mastergrid)\r\n\r\n\r\nThen, we insert the population estimates into the raster based on the\r\ncell IDs of the predicted values.\r\n\r\n\r\nraster_predict[cells] <- mastergrid_predict[cells, 'predicted_pop']\r\n\r\n\r\nIf we are processing the cells in different subsets (i.e. using a for\r\nloop), we can repeat this step on the same raster using different sets\r\nof cell IDs until the raster is fully populated.\r\nFinally, we can save the rasterized results to disk as a geotiff raster.\r\n\r\n\r\nwriteRaster(raster_predict, \r\n            file = 'raster_predict.tif')\r\n\r\n\r\nParallel Processing\r\nParallel processing can improve processing speed when the study area\r\ncontains a large number of spatial units where predictions are required\r\n(i.e. grid cells or polygons). For parallel processing, we will split\r\nthe data into chunks that will be processed simultaneously using all of\r\nthe processing cores available in our computer. The process that we\r\ndescribed above to generate model predictions (see section Weighting\r\nLayer) used only a single processing core even if your computer had\r\nfour or more cores available.\r\nIn the example from the previous section (Gridded Population\r\nEstimates), we saw that Brazil contained more than one billion grid\r\ncells where we would need model predictions. This would likely require\r\nparallel processing to generate model predictions efficiently. For a\r\nsimple and efficient example, we will demonstrate parallel processing\r\nusing the EA-level data in master_predict that will allow you to\r\nexplore the process easily. When you are comfortable with the process,\r\nit can also be applied to the much larger data set in\r\nmastergrid_predict.\r\nFor this task we will need the doParallel R package\r\n(Corporation and Weston 2020) so we will start by installing and loading\r\nthat package.\r\n\r\n\r\ninstall.packages('doParallel')\r\nlibrary('doParallel')\r\n\r\n\r\nWe will need to create a new R function that will generate model\r\npredictions. In the parallel processing steps that will follow, this\r\nfunction will be passed to a processing core along with a chunk of the\r\ndata to generate predictions.\r\n\r\n\r\npredict_pop <- function(df, model=popfit){\r\n  \r\n  # EA-level predictions\r\n  prediction <- predict(model, newdata = df)\r\n  \r\n  # back-transform to population density\r\n  density <- exp(prediction)\r\n  \r\n  # calculate weights\r\n  weights <- density / sum(density) \r\n  \r\n  # disaggregate municipality total to EA-level\r\n  pop <- weights * df$pop_municipality[1]\r\n  \r\n  # result to data.frame\r\n  result <- data.frame(id=df$id, predicted_pop_parallel=pop)\r\n  \r\n  # return result\r\n  return(result)\r\n}\r\n\r\n\r\nThe df argument of the function will be a subset of rows from\r\nmaster_predict and the model argument is the fitted random forest\r\nobject popfit. This function generates model predictions using the\r\npredict function from the randomForest package and then it\r\nback-transforms the predicted log-densities to the natural scale. It\r\nthen calculates weights from those densities and multiplies the weights\r\nby the total population for the municipality that each grid cell belongs\r\nto (note: these are the same steps as in section Redistribution to\r\nEA-level). The results are returned as a data.frame with an “id” column\r\nand a column containing the predictions.\r\nNow, we will add an “id” column into master_predict that we will later\r\nuse to merge the predictions back into this data.frame.\r\n\r\n\r\nmaster_predict$id <- 1:nrow(master_predict)\r\n\r\n\r\nNext, we will break the master_predict data.frame into chunks that\r\nwill be stored as elements in a list object.\r\n\r\n\r\nlist_master_predict <- split(x = master_predict,\r\n                             f = master_predict$geo_code)\r\n\r\n\r\nNotice that we have used the geo_code column in master_predict to\r\ndefine the chunks. Remember, that geo_code identifies each\r\nmunicipality, so we will be processing each municipality separately.\r\nThis simplifies the steps that are required in the predict function\r\n(above) because all units processed in a chunk use the same\r\nmunicipality-level total population (i.e. df$pop_municipality[1]). The\r\nnew object list_master_predict is a list with an element for each\r\nmunicipality that contains a data.frame with a row for each spatial unit\r\nwithin that municipality.\r\nNow, we must setup parallel processing by identifying the number of\r\ncores available, assigning them to a processing cluster, and activating\r\nthe cluster.\r\n\r\n\r\ncores <- detectCores()\r\n  \r\ncluster <- makeCluster(cores)\r\n  \r\nregisterDoParallel(cluster)\r\n\r\n\r\nThen, we can run the predictions in parallel using this cluster.\r\n\r\n\r\npredicted <- foreach(i = 1:length(list_master_predict), \r\n                     .combine = 'rbind',\r\n                     .packages = c(\"randomForest\")) %dopar% \r\n  predict_pop(df = list_master_predict[[i]])\r\n\r\nstopCluster(cluster)\r\n\r\n\r\nThis code will pass each element i from list_master_predict to our\r\npredict function predict_pop() and then combine the resulting\r\ndata.frames using the rbind() function. The last line of code simply\r\ndeactivates our cluster of processing cores.\r\nThe last step is to merge the results back into our original data.frame\r\nmaster_predict using the id column that we setup earlier.\r\n\r\n\r\nmaster_predict <- merge(master_predict,\r\n                        predicted,\r\n                        by = 'id')\r\n\r\n\r\nNote: We have demonstrated parallel processing using the EA-level\r\ndata in master_predict. It is important to note that EA-level\r\npredictions do not take too long to compute using the simpler approach\r\n(see section Weighting Layer). We recommend parallel processing only\r\nfor larger tasks like generating gridded predictions from\r\nmastergrid_predict. We did not use this data in our example because it\r\nwould potentially introduce complications that would distract from the\r\ndemonstration of parallel processing, but the workflow that we\r\ndemonstrated should be easily adaptable to accommodate any larger data\r\nsets.\r\nContributions\r\nThis tutorial was written by Doug Leasure and Edith Darin from the\r\nWorldPop Research Group at the University of Southampton with oversight\r\nfrom Andy Tatem. This work was part of a WorldPop collaboration with the\r\nBrazilian Institute of Geography and Statistics (IBGE), United Nations\r\nPopulation Fund (UNFPA), and the Environmental Systems Research\r\nInstitute (Esri). Attila Lazar and Chris Jochem provided internal\r\nreviews that helped improve the tutorial.\r\n\r\nTutorial’s citation\r\nLeasure DR, Darin E, Tatem AJ. 2020. Small area population estimates\r\nusing random forest top-down disaggregation: An R tutorial. WorldPop,\r\nUniversity of Southampton. doi:10.5258/SOTON/WP00697.\r\n\r\n\r\n\r\nBondarenko, Maksym, Jeremiah Nieves, Alessandro Sorichetta, Forrest R Stevens, Andrea E Gaughan, Andrew Tatem, and others. 2018. wpgpRFPMS: WorldPop Random Forests population modelling R scripts, version 0.1.0. WorldPop, University of Southampton. https://doi.org/10.5258/SOTON/WP00665.\r\n\r\n\r\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324.\r\n\r\n\r\nCorporation, Microsoft, and Steve Weston. 2020. doParallel: Foreach Parallel Adaptor for the ’Parallel’ Package. https://CRAN.R-project.org/package=doParallel.\r\n\r\n\r\nDaniel Baston. 2020. exactextractr: Fast Extraction from Raster Datasets Using Polygons. https://CRAN.R-project.org/package=exactextractr.\r\n\r\n\r\nGenuer, Robin, Jean-Michel Poggi, and Christine Tuleau-Malot. 2010. “Variable Selection Using Random Forests.” Pattern Recognition Letters 31 (14): 2225–36. https://doi.org/10.1016/j.patrec.2010.03.014.\r\n\r\n\r\nHijmans, Robert J. 2020. raster: Geographic Data Analysis and Modeling. https://CRAN.R-project.org/package=raster.\r\n\r\n\r\nIBGE. 2019. Brazilian Territorial Division, 2019 Edition. Brazilian Institute of Geography and Environment (IBGE). https://www.ibge.gov.br/en/geosciences/territorial-organization/regional-division/23708-brazilian-territorial-division.html?=&t=o-que-e.\r\n\r\n\r\n———. 2020a. Meshes of Census Sectors Intra-Municipal Divisions. Brazilian Institute of Geography and Environment (IBGE). http://geoftp.ibge.gov.br/organizacao_do_territorio/malhas_territoriais/malhas_de_setores_censitarios__divisoes_intramunicipais/2019/Malha_de_setores_(shp)_Brasil/.\r\n\r\n\r\n———. 2020b. Population Estimates - Tables 2020. Brazilian Institute of Geography and Environment (IBGE). https://www.ibge.gov.br/en/statistics/social/18448-population-estimates.html?=&t=resultados.\r\n\r\n\r\nLiaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” R News 2 (3): 18–22. https://cran.r-project.org/package=randomForest.\r\n\r\n\r\nLloyd, Christopher T, Heather Chamberlain, David Kerr, Greg Yetman, Linda Pistolesi, Forrest R Stevens, Andrea E Gaughan, et al. 2019. “Global Spatio-Temporally Harmonised Datasets for Producing High-Resolution Gridded Population Distribution Datasets.” Big Earth Data 3 (2): 108–39.\r\n\r\n\r\nLloyd, Christopher T, Alessandro Sorichetta, and Andrew J Tatem. 2017. “High Resolution Global Gridded Data for Use in Population Studies.” Scientific Data 4 (1): 1–17.\r\n\r\n\r\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\r\n\r\n\r\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\r\n\r\n\r\nSorichetta, Alessandro, Graeme M. Hornby, Forrest R. Stevens, Andrea E. Gaughan, Catherine Linard, and Andrew J. Tatem. 2015. “High-resolution gridded population datasets for Latin America and the Caribbean in 2010, 2015, and 2020.” Scientific Data 2 (1): 1–12. https://doi.org/10.1038/sdata.2015.45.\r\n\r\n\r\nStevens, Forrest R., Andrea E. Gaughan, Catherine Linard, and Andrew J. Tatem. 2015. “Disaggregating Census Data for Population Mapping Using Random Forests with Remotely-Sensed and Ancillary Data.” PLOS ONE 10 (2): e0107042. https://doi.org/10.1371/journal.pone.0107042.\r\n\r\n\r\nWorldPop, and CIESIN. 2018a. Administrative Areas: National Boundaries, Brazil. WorldPop, University of Southampton. https://doi.org/10.5258/SOTON/WP00651.\r\n\r\n\r\n———. 2018b. Geospatial covariate data layers: VIIRS night-time lights (2012-216), Brazil. WorldPop, University of Southampton. https://doi.org/10.5258/SOTON/WP00644.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-02-top-down-r-tutorial/./dat/schema.jpg",
    "last_modified": "2025-09-10T17:02:49+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-09-modelling-burkina-faso-population/",
    "title": "Hybrid census in Burkina Faso: Bottom-up population modelling",
    "description": "I developped a Bayesian bottom-up population modelling to adress the challenge of the incomplete 2020 census in Burkina Faso. The model was combining GPS household location with remote sensing product in a Bayesian hierarchcical framework. The output is a hybrid gridded population that was endorsed by Burkina Faso Statistics Office.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2020-09-20",
    "categories": [
      "academia",
      "paper"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nEstimating Missed Communes\r\nModel Method\r\nModel Implementation\r\nInput data\r\n\r\nModel results\r\nImplementing the model\r\nAssessing the model goodness-of-fit\r\n\r\n\r\nEstimating Gridded Population for the Entire Country\r\nModel Method\r\nModel Implementation\r\nInput data\r\n\r\nModel Results\r\nImplementing the model\r\nAssessing the model goodness-of-fit\r\n\r\n\r\nDiscussion\r\nContributions\r\nLicense\r\nSuggested Citation\r\nReferences\r\n\r\n\r\n\r\n\r\nIntroduction\r\nIn February 2020, the Institut National de la Statistique et de la Démographie du Burkina Faso (INSD) completed a census exercise aiming at enumerating the entire country population. Due to security issues in the North and the East regions, some communes (Burkinabe administrative unit level 3) could be only partially covered by governmental surveyors (38 out of 351 communes). The INSD requested support from the GRID3 (Geo-Referenced Infrastructure and Demographic Data for Development) project to estimate the unsurveyed population.\r\nIn December 2020, the INSD released its provisional census population counts for every communes which consisted in estimates for the unsurveyed areas, and census count corrected via a post-enumeration survey for the surveyed areas (Institut National de la Statistique et de la Démographie 2020). Parallel to that traditional publication, the INSD released also a high-resolution gridded population estimates, and its breakdown by age and sex group (WorldPop and Institut National de la Statistique et de la Démographie du Burkina Faso 2021).\r\nThe purpose of this methodological document is to explain:\r\nThe bottom-up modelling used to estimate population totals in incomplete communes\r\nThe top-down modelling used to disaggregate the published census population totals — a combination of estimation-based and enumeration-based commune totals — into high resolution gridded estimates\r\nThe produced dataset (WorldPop and Institut National de la Statistique et de la Démographie du Burkina Faso. 2021) can be downloaded from the WorldPop Open Population Repository (https://wopr.worldpop.org/?BFA/Population/v1.0) and explored on the woprVision data exploration interface (https://apps.worldpop.org/woprVision/).\r\nEstimating Missed Communes\r\nModel Method\r\n\r\n\r\n\r\nFigure 1: Schema for estimating population in missing communes\r\n\r\n\r\n\r\nThe ‘Bottom-up’ modelling followed the hierarchical Bayesian framework developed by Leasure et al. (2020) estimating population totals using sparse survey data. The method combines geospatial covariates available for the entire region of study with observed population data available only for a set of small-size clusters. The estimated relationship is then applied to the grid cells (3 arc-seconds, approximately 100m at the equator) of the unsurveyed areas to predict the population totals. To ensure consistency with the INSD traditional reporting, the gridded estimates were then aggregated at the commune level.\r\nThe hierarchical probabilistic framework enables to leverage statistical relationships between population and geospatial covariates from similar settled locations in order to predict elsewhere and to account for uncertainty in this process. The following equations describe the building blocks of the model:\r\n\\[\r\n\\begin{gathered}\r\n  N_i \\sim Poisson( D_i A_i ) \\\\\r\n  D_i \\sim LogNormal( \\bar{D}_i, \\sigma_{l_1,l_2, l_3}) \\\\\r\n  \\bar{D}_i = \\alpha_{l_1,l_2, l_3} + \\sum_{k=1}^{K} \\beta_k x_{i,k} \\\\\r\n\\\\\r\n\\alpha_{l_1,l_2, l_3} \\sim Normal(11,3)\\\\\r\n\\beta_k \\sim Normal(0,1)\\\\\r\n\\sigma_{l_1,l_2, l_3} \\sim Truncated Normal(0,1,0)\r\n\\end{gathered}\r\n\\] with:\r\na Poisson distribution to model population count \\(N_i\\) in enumeration area \\(i\\)\r\na lognormal distribution to model population density (\\(D_i\\), which is the number of people per settled area in m\\(^2\\), \\(A_i\\))\r\na hierarchical setting for the variance of the lognormal, \\(\\sigma\\) estimated per level \\(l_1, l_2\\) and \\(l_3\\)\r\na mean of the lognormal, \\(\\bar{D}\\) defined deterministically with a hierarchical slope \\(\\alpha_{l_1,l_2,l_3}\\), plus a set of \\(K\\) covariates \\(x_k\\) with related coefficients \\(\\beta_k\\)\r\nModel Implementation\r\nInput data\r\n\r\n\r\n\r\nFigure 2: Population density of the selected enumeration areas\r\n\r\n\r\n\r\nPopulation\r\nThe raw census database consists of the available GPS records for individual and collective resident households. At the time of the analysis, digital Enumeration Areas (EA) were not available, thus for modelling purposes, we compute ad hoc EA boundaries as envelopes around the GPS points belonging to the same enumeration area. A careful selection is then made to remove inaccurate EAs due to error in data collection, EA miss-attribution, imprecise GPS location or inaccurate GPS recording (due to the location where the surveyor entered the observation in the device). We implemented the criteria using quantitative metrics such as: the standard deviation of GPS points for each EA, the number of observations falling in one grid cell, the people/building ratio and the people/settled area ratio.\r\nThe final database used for modelling contains 15 817 EAs which represents 69% of the raw database.\r\nTo prevent overestimating population in unsurveyed communes, we removed from every EAs individuals that were recorded as migrants and originated from the unsurveyed communes. This provides us with a baseline population where displacement from the insecure communes is temporary ignored and tackled at a later stage.\r\nBuilding footprints\r\nA core covariate in our model is the building footprints layer provided by Ecopia.AI and Maxar Technologies (2019). It is a satellite-imagery-based features extraction at 5m and gives a precise estimate of the built-up area in Burkina Faso. Figure 3 illustrates this for an area in Ouagadougou.\r\n\r\n\r\n\r\nFigure 3: Example of the building footprints in Ouagadougou\r\n\r\n\r\n\r\nFrom the building footprints we can calculate the settled area of each EA (i.e. the sum of each building area). During the model fitting, this is used to calculate population densities (people per settled area) from the observed counts.\r\nThe building footprints layer also provides the subset of grid cells for which to predict population count. We predict population only for the grid cells considered as settled, that contains at least one building from the building footprints layer.\r\nGeospatial covariates\r\nTo study Burkina Faso spatial population distribution, we selected 5 covariates from different sources based on their assumed correlation to population densities and on shared range of values between the EA level dataset and the grid cell level dataset.\r\nThe final set of covariates is:\r\nthe count of buildings in a 5km buffer derived from the building footprints layer (Ecopia.AI and Maxar Technologies 2019)\r\nthe distance to temporary rivers and secondary roads provided in the Base Nationale de Données Topographiques (Institut Géographique du Burkina Faso 2015)\r\nthe friction surface used for the Access to Cities project (Weiss, Nelson, Gibson, Temperley, Peedell, Lieber, Hancher, Poyart, Belchior, Fullman, and others 2018a)\r\nthe UN-adjusted unconstrained gridded estimates from WorldPop that disaggregated the Burkina Faso 2019 census projections (WorldPop Research Group, University of Southampton et al. 2018)\r\nFor classifying settlement consistenly with census definition, we used the labelling of each EA in the census database as urban or rural to produce a settlement type prediction at grid cell level.\r\nMore precisely, we used the caret R package (Kuhn et al. 2020) to fit a Gradient Boosting Machine with two covariates, distance to high urban settlement (Institut Géographique du Burkina Faso 2015) and building count in a 500m window (Ecopia.AI and Maxar Technologies 2019). The area under the curve metric of the classification model is 0.98, indicating an almost perfect fit.\r\nModel results\r\nImplementing the model\r\nThe final models is composed of three nested levels for modelling population density: the modelled settlement type, the regions (Burkinabe administrative unit level 1) and the communes.\r\nModel fit was done using the Stan software (Carpenter et al. 2017). Implementing scripts and distribution of estimated parameters can be found on Github: https://github.com/wpgp/BFA_population_v1_0_methods/tree/main/supplements.\r\nTo provide population totals for the unsurveyed communes, we aggregated the gridded bottom-up estimates using official boundaries (Institut Géographique du Burkina Faso 2015). Finally the estimated population totals were corrected for displacement based on the observed census migration status data. We added the migrants that were discarded when processing the census individual database (cf. Section ??) to the commune where they were enumerated and removed them from the unsurveyed commune where they were migrated from.\r\nThe model fit and prediction were done in R version 3.5.1., using the package rstan (Stan Development Team 2020), sf (Pebesma et al. 2020), raster (Hijmans et al. 2020), dplyr (Wickham et al. 2020), data.table (Dowle et al. 2020) and doParallel (Wallig et al. 2020).\r\nAssessing the model goodness-of-fit\r\nPrediction power of the final model is first assessed using a training dataset (70%) to fit the model and a test dataset (30%) to estimate the goodness-of-fit. The correlation between predictions and observations is 0.8. 95% of the test observations are in their 95% confidence interval. Bias (mean of residuals) is 45 people, imprecision (standard deviation of residuals) is 263 people and inaccuracy (mean of absolute residuals) is 169.\r\n\r\n\r\n\r\nFigure 4: Scatterplot of observed vs. predicted population count on test EAs. Red line shows perfect prediction\r\n\r\n\r\n\r\nThe second assessment was undertaken on the population totals for the completed communes, totals that were corrected with omission rates from the post-enumeration survey. This reference data enables us to test the sensitivity of the estimates to the selection process, more precisely two crucial steps:\r\nDrawing EA boundaries around GPS points. We have tested two alternative scenarios:\r\nusing a 25m buffer around the GPS points in Ouagadougou, a 25 m buffer in Saaba et Bobo-Dioulasso and a 100m buffer in the remaining zones;\r\nusing a 20m buffer around the GPS points in Ouagadougou, a 25m buffer in Bobo-Dioulasso and Saaba, a 80m buffer in other urban areas and a 120m buffer in rural areas.\r\n\r\nSelecting the threshold used to discard EA based on the population density. We have tested two alternative options:\r\nfixed national threshold and\r\ncustom thresholds based on the maximum population density per admin 2.\r\n\r\nTo assess the predicted totals, we compute a targeted Root Mean Squared Error for the complete communes nested in incomplete region:\r\n\\[\r\n\\sqrt[][\\frac{1}{n} \\sum_i (\\hat{y}_i - y_i)^2]\r\n\\]\r\nwhere \\(n\\) is the number of complete communes in incomplete regions (\\(n\\)=141), \\(y_i\\) the observed census population totals for commune \\(i\\) and \\(\\hat{y}_i\\) the predicted population totals for commune \\(i\\).\r\n\r\n\r\nTable 1: Table 2: Sensitivity analysis results on predicted totals\r\n\r\n\r\n\r\nSelection procedures\r\n\r\n\r\n\r\n\r\nEA (%)*\r\n\r\n\r\n\r\n\r\nRMSE†\r\n\r\n\r\n\r\nDelineation\r\n\r\n\r\nDensity threshold\r\n\r\n\r\n\r\n\r\n\r\n\r\nScenario 1\r\n\r\n\r\n0.16\r\n\r\n\r\n5\r\n\r\n\r\n12 163\r\n\r\n\r\nScenario 2\r\n\r\n\r\n0.16\r\n\r\n\r\n4\r\n\r\n\r\n10 871\r\n\r\n\r\nScenario 2\r\n\r\n\r\n0.9 x maximum\r\n\r\n\r\n7\r\n\r\n\r\n9 966\r\n\r\n\r\nScenario 2\r\n\r\n\r\n0.8 x maximum\r\n\r\n\r\n9\r\n\r\n\r\n9 745\r\n\r\n\r\n* Additional EAs discarded because of selection procedures (in percentage)\r\n\r\n\r\n† RMSE was compiled over the complete communes from incomplete regions\r\n\r\n\r\nTable 1 shows that differentiating the size of the buffer between urban types does improve the goodness-of-fit. Furthermore choosing a resident density threshold based on the admin 2 maximum resident density succeeds in giving a more representative EA dataset that increase the external goodness-of-fit. Final model is represented in bold.\r\nEstimating Gridded Population for the Entire Country\r\nModel Method\r\n\r\n\r\n\r\nFigure 5: Schema for estimating gridded population for the entire country\r\n\r\n\r\n\r\nTo disaggregate the released communes census count into high-resolution gridded estimates, we adapted the dasymetric mapping explained in Stevens and al. (2015b) using the openly accessible scripts written by Bondarenko and al. (2018). The method consists in modeling population density by combining geospatial and remotely-sensed data in a Random Forest framework. This framework is chosen for its great predictive performance, the absence of complicated tuning parameter and its robustness to multicolinearity and multi-scale issues in the predictors (Robnik-Šikonja 2004).\r\nOnce the model is fitted at administrative level, it is then applied to each grid cell to obtain prediction of population density at a 100m x 100m resolution. This predicted density is then used as a weighting layer to disaggregate census population counts into the settled pixels based on a building footprint dataset from Ecopia.AI and Maxar Technologies (2019). The census counts are thus unevenly distributed across space to better reflect heterogeneous human settlement patterns.\r\nModel Implementation\r\nInput data\r\nPopulation\r\nThe officially released population counts at commune level, displayed in Figure 6 combine the surveyed totals (adjusted with post-enumeration survey non-respondent rates) and the estimated bottom-up totals.\r\n\r\n\r\n\r\nFigure 6: Map of commune population totals\r\n\r\n\r\n\r\nThe census provisional results include also an age and sex decomposition in 18 five-years-groups at national level (Institut National de la Statistique et de la Démographie 2020). These will be used to create the age and sex rasters.\r\nBuilding footprints\r\nThe building footprints layer was already presented in Section ??. For the disagregation exercise we derived four building attributes in addition to building count and settled grid cells, namely:\r\nBuilding area\r\nBuilding perimeter\r\nNearest neighbour distance\r\nNearest neighbour proximity (1/distance)\r\nThe above building attributes are summarised at grid cell level using eight aggregation methods: mean, median (med), standard deviation (sd), mean absolute deviation (mad), minimum, maximum, relative standard deviation (mean/sd) and relative mad (mad/med). Metrics based on the standard deviation or mean absolute deviation indicate the level of heterogeneity of the pixel.\r\nDespite the seemingly similarities between some building-footprints derived covariates, the underlying goal is to increase weak signals for the Random Forest modelling whose predictions will not be impacted by high level correlation (Genuer, Poggi, and Tuleau-Malot 2010).\r\nGeospatial covariates\r\nIn addition to spatial settlement characteristics (contained in the building footprints), we use geospatial covariates to explain the broader context of population distribution.\r\nWe constructed 27 additional covariates from six data sources:\r\n3 from the covariates mapped by the Malaria Atlas Project for their ‘Accessibility to cities’ and ‘Housing in Africa’ projects: travel time in 2015, friction surface in 2015 (Weiss, Nelson, Gibson, Temperley, Peedell, Lieber, Hancher, Poyart, Belchior, Fullman, and others 2018b) and mapping of improved housing conditions in 2015 (Tusting et al. 2019).\r\n4 from the climatic covariates from the Climatic Research Unit: temperature, precipitation, cloud cover and wetness (Harris et al. 2020)\r\n9 from the ESACCI land cover classification (Buchhorn et al. 2020), divided in the 9 overarching classes for which we computed the grid-cell-based distance\r\n3 from the settlement extent (Center for International Earth Science Information Network and Novel-T 2020) classified in 3 types (hamlet, small settlement, built-up areas) for which we computed the grid-cell-based distance\r\n5 from the Base Nationale de Données Topographiques (Institut Géographique du Burkina Faso 2015): the rivers network, the roads network and the settlement points classified in three types according to their urban status (low, middle, high), for which we computed the grid-cell-based distance\r\nthe WorldPop gridded census projection of 2019 (WorldPop Research Group, University of Southampton et al. 2018)\r\nModel Results\r\nImplementing the model\r\nTo model the relationship between census data and the entire set of covariates, we transform the population count into population density by dividing it with the area of all settled grid cells in the commune and then log-transform it (Forrest R. Stevens et al. 2015a). We then average the covariates using zonal mean for every communes. The procedure developed in Bondarenko et al. (2018) was used to both tune the Random Forest model and select covariates using their importance score to speed up the per-pixel prediction.\r\nOnce the model is fitted at commune level, we use it to predict the population density for each grid cell. This predicted density is then used as a weight to disaggregate the total population count for each commune. The disaggregated gridded population estimates are multiplied by the age and sex proportions from the national demographic pyramid to estimate population counts per age and sex group for each grid cell.\r\nThe code for the model fit and prediction can be found on Github: https://github.com/wpgp/BFA_population_v1_0_methods\r\nThe fit and the prediction were done in R version 3.5.1., using the package randomForest (Cutler and Wiener 2018), sf_0.9-5 (Pebesma et al. 2020), raster (Hijmans et al. 2020), dplyr (Wickham et al. 2020), data.table (Dowle et al. 2020) and doParallel (Wallig et al. 2020).\r\nAssessing the model goodness-of-fit\r\nCovariates importance We assess the importance of a predictor by showing the relative increase in the mean square error when the predictor is removed from the model. The order of magnitude might not be robust to the collinearity observed in the covariates set but the relative ranking is (Genuer, Poggi, and Tuleau-Malot 2010).\r\n\r\n\r\n\r\nFigure 7: Top 25 Covariates importance in the Random Forest model\r\n\r\n\r\n\r\nFigure 7 shows that the mean building count per pixel is by far the most important predictor, followed by the gridded population projections from WorldPop that informs population distribution from previous censuses. The presence in the top 25 covariates of different land cover classes confirms previous findings on the predictive power of land classification (Lloyd et al. 2019).\r\nAn interesting additional feature is the strong predictive power of building-derived metric depicting heterogeneity in a grid cell (standard deviation -sd-, mean absolute deviation -mad-, coefficient of variation -cv- and relative mean absolute deviation -mdcv-).\r\nAssessing the model A metric that is commonly used to assess a Random Forest model is the percentage of variance explained (Liaw and Wiener 2002). It corresponds to the “out-of-bag” mean squared error divided by the variance of the observations: \\[1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\\]whre \\(y_i\\) is the observed population totals for commune \\(i\\) with \\(\\bar{y}\\) its corresponding average and \\(\\hat{y}_i\\) is the predicted population totals for commune \\(i\\) on a test dataset.\r\nIn our model the percentage of variance explained is equal to 70.1%. For comparison the model developed by Stevens et al. (2015b) for Burkina Faso using census projections and a standardised set of covariates (Lloyd et al. 2019) had a percentage of variance explained of 59.1%1.\r\nThe difference is explained by the custom set of covariates used in our model, especially the one derived from the building footprints, and the restriction of population estimation and prediction to the settled grid cells.\r\nAssessing the prediction In Stevens and al. (2015b) external validation was undertaken by applying the modelling process to a coarser census scale and then aggregating predicted population per pixel at the finer census unit to compare it with census results. However in Burkina Faso, the coarser administrative unit after the commune is the province level that has only 45 units. We considered this sample size too small to perform a meaningful model assessment of the Random Forest modelling.\r\nNonetheless, we had access to the raw census database where household GPS points provide a geo-tagging of enumeration areas across the country. Thus, this oﬀers the possibility to check the model performance at a fine spatial scale. We manually selected a subset of 50 enumeration areas across the country in diﬀerent settlement settings based on the visual assessment of the satellite imagery. We then computed a predicted population by summing up the corresponding grid cells and performed a comparison analysis based on the relative prediction error (Figure 8 and 9):\r\n\\[\r\n\\frac{\\hat{y} - y}{y}*100\r\n\\]\r\nwhere \\(\\hat{y}\\) is the predicted and \\(y\\) the observed EA population count.\r\n\r\n\r\n\r\nFigure 8: Spatial distribution of the relative prediction errors. Dots shows the location of the selected EAs. The colours show the ranges of predictive errors. Settled pixels are shown in yellow.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 9: Distribution of the relative prediction errors.Grey bars represent each of the 50 selected EAs.The orange line shows the location of the 0% error, the black line summarises the overall error distribution.\r\n\r\n\r\n\r\nFigure 9 shows an average bias of the prediction of -3%. Imprecision (std. of relative error) is 27% and inaccuracy 21% (mean of the absolute relative error).\r\nThe greater prediction errors are occurring in rural area as shown by the spatial distribution of the settled cells without clear geographical clustering (Figure 8).\r\nDiscussion\r\nThe assumptions and limitations of this study can be summarised by topics.\r\nCountry extent Administrative boundary datasets include the entire surface of the country. Any settled grid cells outside of the boundaries from the Base Nationale de Données Topographiques (Institut Géographique du Burkina Faso 2015) were not included.\r\nData date Although the input data have varying reference years, the estimates should be considered as representing late 2019 as thisis the date of the census data collection.\r\nSettlement type The population counts in areas with primarily non-residential buildings may be over-estimated. Caution should be taken when using the population data in industrial (and other primarily non-residential) areas. Furthermore little variation can be modelled for densely populated urban areas. This is due to the scale of the input data used to fit the model, namely at administrative level 3, that does not capture well variation in a dense city such as Ouagadougou. Therefore precautions shall be taken when focusing the analysis in urban zones.\r\nBuilding footprints We assume that the building footprints data is accurate and that each building polygon corresponds to a building structure. However the date of the satellite imagery spans from 2009 to 2019 (see Figure 10). The estimated population counts may be inaccurate in areas where the imagery is old and buildings have recently been constructed. This issue was observed in the towns of Zorgho, Bâtié and Nanoro. But it is minimized by old satellite imagery being only used in remote area.\r\n\r\n\r\n\r\nFigure 10: Satellite imagery date used for extracting building footprints by Ecopia.AI, and Maxar Technologies. (2019)\r\n\r\n\r\n\r\nDiscrete population pattern Administrative boundaries are visible in the disagregated estimates whereby sharp population differences can be found for neighboring grid cells along the administrative unit delimitation. This is the consequence of the dasymetric top-down disagregation which ensures that the sum of every grid cells of the administrative unit does match with the total provided.\r\nModeled population The population that is predicted is the de jure resident population as defined by the INSD, that is planning in staying or having stayed already six months at the place where the census is taking place. This population dataset represent neither the ambient population, nor captures internal migration below the commune level.\r\nAge and sex pyramid The age and sex proportions are released by the INSD at the national level, therefore no subnational variations in proportion were considered for the gridded estimates.\r\nContributions\r\nThis work is part of the GRID3 (Geo-Referenced Infrastructure and Demographic Data for Development) project funded by the Bill and Melinda Gates Foundation (BMGF) and the United Kingdom’s Department for International Development (OPP1182408). Project partners include WorldPop at the University of Southampton, the United Nations Population Fund (UNFPA), Center for International Earth Science Information Network (CIESIN) in the Earth Institute at Columbia University, and the Flowminder Foundation. The Institut National de la Statistique et de la Démographie supported, facilitated this work, reviewed the results and provided the census database. The modelling work, geospatial data processing, stakeholder engagement and model report was led by Edith Darin with help from Mathias Kuépié. Support for the statistical modelling was provided by Gianluca Boo, Claire A. Dooley, Douglas R. Leasure and Chris W. Jochem. Gianluca Boo, Douglas R. Leasure and Attila N. Lazar provided a thorough review of the manuscript. Oversight was done by Andrew J. Tatem and Attila N. Lazar.\r\nLicense\r\nThis report may be redistributed using a Creative Commons Attribution 4.0 International (CC BY 4.0) License.\r\nSuggested Citation\r\nWorldPop and Institut National de la Statistique et de la Démographie du Burkina Faso. 2021. Census-based gridded population estimates for Burkina Faso (2019), version 1.0. WorldPop, University of Southampton. doi:10.5258/SOTON/WP00687.\r\nReferences\r\n\r\n\r\n\r\nBondarenko, Maksym, Jeremiah Nieves, Alessandro Sorichetta, Forrest R Stevens, Andrea E Gaughan, Andrew Tatem, et al. 2018. “wpgpRFPMS: WorldPop Random Forests Population Modelling r Scripts, Version 0.1. 0.”\r\n\r\n\r\nBuchhorn, Marcel, Bruno Smets, Luc Bertels, Bert De Roo, Myroslava Lesiv, Nandin-Erdene Tsendbazar, Martin Herold, and Steffen Fritz. 2020. “Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe.” Zenodo. https://doi.org/10.5281/zenodo.3939050.\r\n\r\n\r\nCarpenter, Bob, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. “Stan: A Probabilistic Programming Language.” Journal of Statistical Software 76 (1). https://doi.org/10.18637/jss.v076.i01.\r\n\r\n\r\nCenter for International Earth Science Information Network, and Novel-T. 2020. “GRID3 Burkina Faso Settlement Extents Version 01, Alpha.” https://doi.org/10.7916/d8-h47k-8637.\r\n\r\n\r\nCutler, Fortran original by Leo Breiman and Adele, and R. port by Andy Liaw and Matthew Wiener. 2018. randomForest: Breiman and Cutler’s Random Forests for Classification and Regression. https://CRAN.R-project.org/package=randomForest.\r\n\r\n\r\nDowle, Matt, Arun Srinivasan, Jan Gorecki, Michael Chirico, Pasha Stetsenko, Tom Short, Steve Lianoglou, et al. 2020. Data.table: Extension of ’Data.frame’. https://CRAN.R-project.org/package=data.table.\r\n\r\n\r\nEcopia.AI, and Maxar Technologies. 2019. Digitize Africa Data.\r\n\r\n\r\nGenuer, Robin, Jean-Michel Poggi, and Christine Tuleau-Malot. 2010. “Variable Selection Using Random Forests.” Pattern Recognition Letters 31 (14): 2225–36.\r\n\r\n\r\nHarris, Ian, Timothy J Osborn, Phil Jones, and David Lister. 2020. “Version 4 of the CRU TS Monthly High-Resolution Gridded Multivariate Climate Dataset.” Scientific Data 7 (1): 1–18.\r\n\r\n\r\nHijmans, Robert J., Jacob van Etten, Michael Sumner, Joe Cheng, Dan Baston, Andrew Bevan, Roger Bivand, et al. 2020. Raster: Geographic Data Analysis and Modeling. https://CRAN.R-project.org/package=raster.\r\n\r\n\r\nInstitut Géographique du Burkina Faso. 2015. “Base Nationale de Données Topographiques.”\r\n\r\n\r\nInstitut National de la Statistique et de la Démographie. 2020. “Cinquième Recensement Général de la Population et de l’Habitation du Burkina Faso - Résultats préliminaires.” INSD Ouagadougou, Burkina Faso.\r\n\r\n\r\nKuhn, Max, Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, et al. 2020. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret.\r\n\r\n\r\nLeasure, Douglas R, Warren C Jochem, Eric M Weber, Vincent Seaman, and Andrew J Tatem. 2020. “National Population Mapping from Sparse Survey Data: A Hierarchical Bayesian Modeling Framework to Account for Uncertainty.” Proceedings of the National Academy of Sciences.\r\n\r\n\r\nLiaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” R News 2 (3): 18–22. https://CRAN.R-project.org/doc/Rnews/.\r\n\r\n\r\nLloyd, Christopher T, Heather Chamberlain, David Kerr, Greg Yetman, Linda Pistolesi, Forrest R Stevens, Andrea E Gaughan, et al. 2019. “Global Spatio-Temporally Harmonised Datasets for Producing High-Resolution Gridded Population Distribution Datasets.” Big Earth Data 3 (2): 108–39. https://doi.org/10.1080/20964471.2019.1625151.\r\n\r\n\r\nPebesma, Edzer, Roger Bivand, Etienne Racine, Michael Sumner, Ian Cook, Tim Keitt, Robin Lovelace, et al. 2020. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\r\n\r\n\r\nRobnik-Šikonja, Marko. 2004. “Improving Random Forests.” In European Conference on Machine Learning, 359–70. Springer.\r\n\r\n\r\nStan Development Team. 2020. “RStan: The R Interface to Stan.” http://mc-stan.org/.\r\n\r\n\r\nStevens, Forrest R., Andrea E. Gaughan, Catherine Linard, and Andrew J. Tatem. 2015a. “Disaggregating Census Data for Population Mapping Using Random Forests with Remotely-Sensed and Ancillary Data.” PLOS ONE 10 (2): e0107042. https://doi.org/10.1371/journal.pone.0107042.\r\n\r\n\r\nStevens, Forrest R, Andrea E Gaughan, Catherine Linard, and Andrew J Tatem. 2015b. “Disaggregating Census Data for Population Mapping Using Random Forests with Remotely-Sensed and Ancillary Data.” PloS One 10 (2). https://doi.org/10.1371/journal.pone.0107042.\r\n\r\n\r\nTusting, Lucy S, Donal Bisanzio, Graham Alabaster, Ewan Cameron, Richard Cibulskis, Michael Davies, Seth Flaxman, et al. 2019. “Mapping Changes in Housing in Sub-Saharan Africa from 2000 to 2015.” Nature 568 (7752): 391–94.\r\n\r\n\r\nWallig, Michelle, Microsoft Corporation, Steve Weston, and Dan Tenenbaum. 2020. doParallel: Foreach Parallel Adaptor for the ’Parallel’ Package. https://CRAN.R-project.org/package=doParallel.\r\n\r\n\r\nWeiss, D J, A Nelson, HS Gibson, W Temperley, S Peedell, A Lieber, M Hancher, E Poyart, S Belchior, N Fullman, and others. 2018b. “A Global Map of Travel Time to Cities to Assess Inequalities in Accessibility in 2015.” Nature 553 (7688): 333–36.\r\n\r\n\r\nWeiss, D J, A Nelson, HS Gibson, W Temperley, S Peedell, A Lieber, M Hancher, E Poyart, S Belchior, N Fullman, and others. 2018a. “A Global Map of Travel Time to Cities to Assess Inequalities in Accessibility in 2015.” Nature 553 (7688): 333336.\r\n\r\n\r\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and RStudio. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\r\n\r\n\r\nWorldPop Research Group, University of Southampton, Department of Geography and Geosciences, University of Louisville, Departement de Geographie, Universite de Namur, and Center for International Earth Science Information Network (CIESIN), Columbia University. 2018. “Global High Resolution Population Denominators Project - Funded by the Bill and Melinda Gates Foundation (OPP1134076).” https://doi.org/10.5258/SOTON/WP00645.\r\n\r\n\r\nWorldPop, and Institut National de la Statistique et de la Démographie du Burkina Faso. 2021. “Census-Based Gridded Population Estimates for Burkina Faso (2019), Version 1.0.” WorldPop, University of Southampton. https://doi.org/10.5258/SOTON/WP00687.\r\n\r\n\r\nSee Burkina Faso 2019 in http://dx.doi.org/10.5258/SOTON/WP00645 for more details.↩︎\r\n",
    "preview": "posts/2021-07-09-modelling-burkina-faso-population/./dat/resident_density_mean_admin3.png",
    "last_modified": "2025-09-10T17:16:58+01:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 796
  },
  {
    "path": "posts/2020-09-01-coopcycle/",
    "title": "CoopCycle: we socialize platform",
    "description": "Since 2017 I am a vice-president of the CoopCycle association  that fights for fair working rights in the bike delivery sector.",
    "author": [
      {
        "name": "Edith Darin",
        "url": {}
      }
    ],
    "date": "2020-09-01",
    "categories": [
      "cooperative"
    ],
    "contents": "\r\n\r\nCoopCycle is an association that aims at creating an European federation of local bike delivery cooperative to mutualize costs and share the knowledge and lessons learned of a worker-owned delivery activity. The services offered by the network consist in developing a tailored platform/dispatch software that answers the needs of riders, advocating around bike delivery at different scales, applying for grants application or providing adivce on legislation, worker rights, marketing and branding.\r\nWe also created a custom licensing to restrict software access only to workers co-ops where production and economic value are democratically shared. Our objective is to allow couriers to build co-ops and then emancipate themselves from existing platforms (Deliveroo and co).\r\nIn 2021, CoopCycle gathered 65 collectives across three continents and six countries.\r\nWe won the Paris City Hall prize for Social Business (30k) and Call for Action in Seine-Saint-Denis (8k).\r\nTo learn more about the adventure: https://coopcycle.org/en/\r\nMy contributions\r\nAs a member I’m involved at different stages of the projet from working group animation, grants application, and local coop support to meetings organization and outreach activities.\r\nTalks\r\n“CoopCycle as a Showcase for Platform Cooperativsm”. New-York Platform Cooperativism Consortium 2019\r\n“How couriers can take control: A co-op alternative to Uber Eats and Deliveroo”, Manchester Co-operatives UK. \\[Novembre 2018\\] Video\r\n“CoopCycle as a Showcase for Platform Cooperativsm”. Hong-Kong Platform Cooperativism Consortium 2018 \\[Septembre 2018\\] Video\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-09-01-coopcycle/coopcycle_logo.jpg",
    "last_modified": "2025-09-10T16:57:17+01:00",
    "input_file": {}
  }
]
